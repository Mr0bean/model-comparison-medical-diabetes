"""
äº¤å‰è¯„ä¼°å¼•æ“
è´Ÿè´£æ‰§è¡Œæ¨¡å‹é—´çš„äº¤å‰è¯„ä¼°
"""

import json
import time
from pathlib import Path
from typing import Dict, List, Optional, Tuple
from datetime import datetime
import sys
from tqdm import tqdm

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„ä»¥ä¾¿å¯¼å…¥chat_client
sys.path.insert(0, str(Path(__file__).parent.parent))

from chat_client import ChatClient, Message
from .config import EVALUATION_CONFIG, API_CONFIG, OUTPUT_CONFIG
from .prompt_template import EvaluationPromptTemplate
from .conversation_indexer import ConversationIndexer
from .model_client_factory import ModelClientFactory


class CrossEvaluationEngine:
    """äº¤å‰è¯„ä¼°å¼•æ“"""

    def __init__(
        self,
        comparison_data_path: str = "output/comparison_data.json",
        output_base_dir: str = None,
        markdown_reports_dir: str = "output/markdown"
    ):
        """
        åˆå§‹åŒ–è¯„ä¼°å¼•æ“

        Args:
            comparison_data_path: æ¨¡å‹å¯¹æ¯”æ•°æ®è·¯å¾„
            output_base_dir: è¾“å‡ºåŸºç¡€ç›®å½•
            markdown_reports_dir: MarkdownæŠ¥å‘Šæ–‡ä»¶ç›®å½•
        """
        self.comparison_data_path = Path(comparison_data_path)
        self.output_base_dir = Path(output_base_dir or OUTPUT_CONFIG["base_dir"])
        self.markdown_reports_dir = Path(markdown_reports_dir)

        # åŠ è½½å¯¹æ¯”æ•°æ®
        self.comparison_data = self._load_comparison_data()

        # åˆå§‹åŒ–å¯¹è¯ç´¢å¼•å™¨
        self.conversation_indexer = ConversationIndexer()
        self.conversation_indexer.load_all_conversations()

        # åˆå§‹åŒ–promptæ¨¡æ¿ç”Ÿæˆå™¨
        self.prompt_template = EvaluationPromptTemplate()

        # åˆå§‹åŒ–æ¨¡å‹å®¢æˆ·ç«¯å·¥å‚
        self.client_factory = ModelClientFactory()

        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_base_dir.mkdir(parents=True, exist_ok=True)

    def _load_comparison_data(self) -> Dict:
        """åŠ è½½å¯¹æ¯”æ•°æ®"""
        with open(self.comparison_data_path, 'r', encoding='utf-8') as f:
            return json.load(f)

    def _load_markdown_report(self, model: str, patient: str) -> Optional[str]:
        """
        åŠ è½½markdownæŠ¥å‘Šæ–‡ä»¶

        Args:
            model: æ¨¡å‹åç§°ï¼ˆå¦‚ 'gpt-5.1'ï¼‰
            patient: æ‚£è€…åç§°ï¼ˆå¦‚ 'æ‚£è€…1'ï¼‰

        Returns:
            æŠ¥å‘Šå†…å®¹ï¼Œå¦‚æœæ–‡ä»¶ä¸å­˜åœ¨æˆ–è¯»å–å¤±è´¥è¿”å›None
        """
        try:
            # æ ‡å‡†åŒ–æ¨¡å‹åç§°ï¼ˆç”¨äºæ–‡ä»¶åï¼‰
            model_filename = model.replace('/', '_')

            # æ„å»ºæ–‡ä»¶è·¯å¾„ï¼š{model}-{patient}.md
            report_file = self.markdown_reports_dir / f"{model_filename}-{patient}.md"

            if not report_file.exists():
                return None

            with open(report_file, 'r', encoding='utf-8') as f:
                content = f.read()

            if not content.strip():
                print(f"  âš ï¸  æŠ¥å‘Šæ–‡ä»¶ä¸ºç©º: {report_file}")
                return None

            return content

        except Exception as e:
            print(f"  âœ— è¯»å–æŠ¥å‘Šå¤±è´¥: {model}-{patient}.md - {e}")
            return None

    def run_report_cross_evaluation(
        self,
        æ‚£è€…åˆ—è¡¨: List[str] = None,
        æ¨¡å‹åˆ—è¡¨: List[str] = None
    ) -> Dict:
        """
        è¿è¡Œå®Œæ•´æŠ¥å‘Šçš„äº¤å‰è¯„ä¼°ï¼ˆä¸æŒ‰å¯¹è¯åˆ†æ®µï¼‰

        Args:
            æ‚£è€…åˆ—è¡¨: è¦è¯„ä¼°çš„æ‚£è€…åˆ—è¡¨ï¼ŒNoneè¡¨ç¤ºå…¨éƒ¨
            æ¨¡å‹åˆ—è¡¨: è¦ä½¿ç”¨çš„æ¨¡å‹åˆ—è¡¨ï¼ŒNoneè¡¨ç¤ºå…¨éƒ¨

        Returns:
            è¯„ä¼°ç»“æœæ±‡æ€»
        """
        print("\n" + "="*60)
        print("å®Œæ•´æŠ¥å‘Šäº¤å‰è¯„ä¼°å¼•æ“å¯åŠ¨")
        print("="*60 + "\n")

        # æ˜¾ç¤ºæ¨¡å‹æ³¨å†Œä¿¡æ¯
        print("å·²æ³¨å†Œçš„æ¨¡å‹åŠå…¶APIæä¾›å•†:")
        registered_models = self.client_factory.list_available_models()
        for model, provider in registered_models.items():
            print(f"  - {model}: {provider}")
        print()

        # ç¡®å®šè¯„ä¼°èŒƒå›´
        patients = æ‚£è€…åˆ—è¡¨ or self.comparison_data.get("patients", [])
        models = æ¨¡å‹åˆ—è¡¨ or self.comparison_data.get("models", [])

        print(f"è¯„ä¼°èŒƒå›´:")
        print(f"  - æ‚£è€…æ•°é‡: {len(patients)}")
        print(f"  - æ¨¡å‹æ•°é‡: {len(models)}")
        print(f"  - è¯„ä¼°çŸ©é˜µå¤§å°: {len(models)} Ã— {len(models)}")
        total_evals = len(patients) * len(models) * (len(models) - 1) if not EVALUATION_CONFIG["include_self_evaluation"] else len(patients) * len(models) * len(models)
        print(f"  - é¢„è®¡è¯„ä¼°æ¬¡æ•°: {total_evals}\n")

        results = {
            "start_time": datetime.now().isoformat(),
            "patients": patients,
            "models": models,
            "evaluations": {}
        }

        total_evaluations = 0
        successful_evaluations = 0
        failed_evaluations = 0

        # è®¡ç®—æ€»ä»»åŠ¡æ•°
        total_tasks = 0
        for patient in patients:
            for generated_by_model in models:
                for evaluator_model in models:
                    if not EVALUATION_CONFIG["include_self_evaluation"] and generated_by_model == evaluator_model:
                        continue
                    total_tasks += 1

        # åˆ›å»ºæ€»è¿›åº¦æ¡
        with tqdm(total=total_tasks, desc="ğŸ”„ æ€»ä½“è¿›åº¦",
                  bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]',
                  ncols=100) as pbar:

            # éå†æ¯ä¸ªæ‚£è€…
            for patient_idx, patient in enumerate(patients, 1):
                try:
                    pbar.set_description(f"ğŸ“‹ æ‚£è€… {patient_idx}/{len(patients)}: {patient}")

                    # éå†æ¯ä¸ªè¢«è¯„ä¼°çš„æ¨¡å‹(æŠ¥å‘Šç”Ÿæˆè€…)
                    for gen_idx, generated_by_model in enumerate(models, 1):
                        try:
                            # è¯»å–è¯¥æ¨¡å‹ç”Ÿæˆçš„å®Œæ•´æŠ¥å‘Š
                            report_content = self._load_markdown_report(generated_by_model, patient)

                            if not report_content:
                                tqdm.write(f"  âš ï¸  æœªæ‰¾åˆ°æŠ¥å‘Š: {generated_by_model}-{patient}.md")
                                # è·³è¿‡è¿™ä¸ªæ¨¡å‹çš„æ‰€æœ‰è¯„ä¼°
                                skip_count = len(models)
                                if not EVALUATION_CONFIG["include_self_evaluation"]:
                                    skip_count -= 1
                                pbar.update(skip_count)
                                continue

                            # è®©æ¯ä¸ªæ¨¡å‹è¿›è¡Œè¯„ä¼°
                            for eval_idx, evaluator_model in enumerate(models, 1):
                                try:
                                    total_evaluations += 1

                                    # æ£€æŸ¥æ˜¯å¦è·³è¿‡è‡ªæˆ‘è¯„ä¼°
                                    if not EVALUATION_CONFIG["include_self_evaluation"] and generated_by_model == evaluator_model:
                                        pbar.update(1)
                                        continue

                                    # æ›´æ–°è¿›åº¦æ¡æè¿°
                                    pbar.set_postfix_str(
                                        f"ç”Ÿæˆè€…:{generated_by_model[:15]}... | è¯„ä¼°è€…:{evaluator_model[:15]}..."
                                    )

                                    # æ£€æŸ¥ç¼“å­˜ï¼ˆæ£€æŸ¥evaluationsæ–‡ä»¶ï¼Œå› ä¸ºè¿™æ˜¯æœ€ç»ˆç»“æœï¼‰
                                    eval_file = self._get_report_evaluation_file_path(
                                        patient, generated_by_model, evaluator_model
                                    )

                                    if EVALUATION_CONFIG["enable_caching"] and eval_file.exists():
                                        try:
                                            # è¯»å–å·²æœ‰çš„è¯„åˆ†
                                            with open(eval_file, 'r', encoding='utf-8') as f:
                                                cached_result = json.load(f)
                                                avg_score = cached_result.get("average_score", 0)
                                            tqdm.write(f"    âœ“ å·²ç¼“å­˜: {evaluator_model} (è¯„åˆ†: {avg_score:.2f})")
                                            successful_evaluations += 1
                                            pbar.update(1)
                                            continue
                                        except Exception as cache_error:
                                            tqdm.write(f"    âš ï¸  ç¼“å­˜è¯»å–å¤±è´¥: {cache_error}ï¼Œé‡æ–°è¯„ä¼°")

                                    # æ‰§è¡Œè¯„ä¼°ï¼ˆä¸¤æ­¥èµ°ï¼šå…ˆä¿å­˜åŸå§‹å“åº”ï¼Œå†è§£æå¹¶ä¿å­˜è¯„åˆ†ï¼‰
                                    try:
                                        # ç¬¬ä¸€æ­¥ï¼šè·å–å¹¶ä¿å­˜åŸå§‹å“åº”
                                        raw_response = self._get_raw_report_evaluation(
                                            patient=patient,
                                            generated_by=generated_by_model,
                                            evaluated_by=evaluator_model,
                                            report_content=report_content
                                        )

                                        # ç«‹å³ä¿å­˜åŸå§‹å“åº”
                                        raw_file = self._save_report_raw_response(
                                            patient, generated_by_model,
                                            evaluator_model, raw_response
                                        )

                                        # ç¬¬äºŒæ­¥ï¼šå°è¯•è§£æå¹¶ä¿å­˜è¯„åˆ†ç»“æœ
                                        try:
                                            evaluation_result = self._parse_and_save_report_evaluation(
                                                patient, generated_by_model,
                                                evaluator_model, raw_response
                                            )

                                            avg_score = evaluation_result.get("average_score", 0)
                                            tqdm.write(f"    âœ“ å®Œæˆ: {evaluator_model} (è¯„åˆ†: {avg_score:.2f})")

                                        except Exception as parse_error:
                                            tqdm.write(f"    âš ï¸  è§£æå¤±è´¥: {evaluator_model} - {parse_error}")
                                            tqdm.write(f"       åŸå§‹å“åº”å·²ä¿å­˜: {raw_file}")

                                        successful_evaluations += 1

                                        # é¿å…APIé™æµ
                                        time.sleep(API_CONFIG.get("retry_delay", 1))

                                    except Exception as eval_error:
                                        tqdm.write(f"    âœ— è¯„ä¼°å¤±è´¥: {evaluator_model} - {eval_error}")
                                        failed_evaluations += 1

                                    finally:
                                        # æ— è®ºæˆåŠŸå¤±è´¥éƒ½æ›´æ–°è¿›åº¦
                                        pbar.update(1)

                                except Exception as eval_loop_error:
                                    tqdm.write(f"    âœ— è¯„ä¼°å¾ªç¯å¼‚å¸¸: {evaluator_model} - {eval_loop_error}")
                                    failed_evaluations += 1
                                    pbar.update(1)

                        except Exception as gen_loop_error:
                            tqdm.write(f"  âœ— ç”Ÿæˆè€…å¾ªç¯å¼‚å¸¸: {generated_by_model} - {gen_loop_error}")
                            # è·³è¿‡å‰©ä½™çš„è¯„ä¼°è€…
                            remaining = len(models) - gen_idx + 1
                            if not EVALUATION_CONFIG["include_self_evaluation"]:
                                remaining -= 1
                            pbar.update(max(0, remaining))

                except Exception as patient_loop_error:
                    tqdm.write(f"âœ— æ‚£è€…å¾ªç¯å¼‚å¸¸: {patient} - {patient_loop_error}")
                    # è·³è¿‡è¯¥æ‚£è€…çš„å‰©ä½™ä»»åŠ¡
                    remaining = (len(patients) - patient_idx) * len(models) * len(models)
                    if not EVALUATION_CONFIG["include_self_evaluation"]:
                        remaining -= (len(patients) - patient_idx) * len(models)
                    pbar.update(max(0, remaining))

        results["end_time"] = datetime.now().isoformat()
        results["statistics"] = {
            "total_evaluations": total_evaluations,
            "successful": successful_evaluations,
            "failed": failed_evaluations,
            "success_rate": f"{successful_evaluations/total_evaluations*100:.1f}%" if total_evaluations > 0 else "0%"
        }

        print("\n" + "="*60)
        print("äº¤å‰è¯„ä¼°å®Œæˆ")
        print("="*60)
        print(f"æ€»è¯„ä¼°æ¬¡æ•°: {total_evaluations}")
        print(f"æˆåŠŸ: {successful_evaluations}")
        print(f"å¤±è´¥: {failed_evaluations}")
        print(f"æˆåŠŸç‡: {results['statistics']['success_rate']}")
        print("="*60 + "\n")

        return results

    def run_cross_evaluation(
        self,
        æ‚£è€…åˆ—è¡¨: List[str] = None,
        å¯¹è¯ç±»å‹åˆ—è¡¨: List[str] = None,
        æ¨¡å‹åˆ—è¡¨: List[str] = None,
        include_conversation_context: bool = True
    ) -> Dict:
        """
        è¿è¡Œäº¤å‰è¯„ä¼°

        Args:
            æ‚£è€…åˆ—è¡¨: è¦è¯„ä¼°çš„æ‚£è€…åˆ—è¡¨ï¼ŒNoneè¡¨ç¤ºå…¨éƒ¨
            å¯¹è¯ç±»å‹åˆ—è¡¨: è¦è¯„ä¼°çš„å¯¹è¯ç±»å‹IDåˆ—è¡¨ï¼ŒNoneè¡¨ç¤ºå…¨éƒ¨
            æ¨¡å‹åˆ—è¡¨: è¦ä½¿ç”¨çš„æ¨¡å‹åˆ—è¡¨ï¼ŒNoneè¡¨ç¤ºå…¨éƒ¨
            include_conversation_context: æ˜¯å¦åœ¨è¯„ä¼°æ—¶åŒ…å«åŸå§‹å¯¹è¯ä¸Šä¸‹æ–‡

        Returns:
            è¯„ä¼°ç»“æœæ±‡æ€»
        """
        print("\n" + "="*60)
        print("äº¤å‰è¯„ä¼°å¼•æ“å¯åŠ¨")
        print("="*60 + "\n")

        # æ˜¾ç¤ºæ¨¡å‹æ³¨å†Œä¿¡æ¯
        print("å·²æ³¨å†Œçš„æ¨¡å‹åŠå…¶APIæä¾›å•†:")
        registered_models = self.client_factory.list_available_models()
        for model, provider in registered_models.items():
            print(f"  - {model}: {provider}")
        print()

        # ç¡®å®šè¯„ä¼°èŒƒå›´
        patients = æ‚£è€…åˆ—è¡¨ or self.comparison_data.get("patients", [])
        models = æ¨¡å‹åˆ—è¡¨ or self.comparison_data.get("models", [])

        print(f"è¯„ä¼°èŒƒå›´:")
        print(f"  - æ‚£è€…æ•°é‡: {len(patients)}")
        print(f"  - æ¨¡å‹æ•°é‡: {len(models)}")
        print(f"  - è¯„ä¼°çŸ©é˜µå¤§å°: {len(models)} Ã— {len(models)}")
        print(f"  - é¢„è®¡è¯„ä¼°æ¬¡æ•°: {len(patients) * len(models) * len(models)} (æ‚£è€… Ã— ç”Ÿæˆè€… Ã— è¯„ä¼°è€…)\n")

        results = {
            "start_time": datetime.now().isoformat(),
            "patients": patients,
            "models": models,
            "evaluations": {}
        }

        total_evaluations = 0
        successful_evaluations = 0
        failed_evaluations = 0

        # éå†æ¯ä¸ªæ‚£è€…
        for patient in patients:
            print(f"\nå¤„ç†æ‚£è€…: {patient}")
            print("-" * 60)

            patient_data = self.comparison_data["data"].get(patient, {})

            # ç¡®å®šè¦è¯„ä¼°çš„å¯¹è¯ç±»å‹
            conv_ids = å¯¹è¯ç±»å‹åˆ—è¡¨ or list(patient_data.keys())

            for conv_id in conv_ids:
                conv_data = patient_data.get(conv_id, {})
                if not conv_data:
                    continue

                # è·å–å¯¹è¯æ ‡é¢˜
                conv_title = self._get_conversation_title(conv_data)
                print(f"\n  å¯¹è¯ç±»å‹: {conv_title} (ID: {conv_id})")

                # éå†æ¯ä¸ªè¢«è¯„ä¼°çš„æ¨¡å‹(ç”Ÿæˆè€…)
                for generated_by_model in models:
                    if generated_by_model not in conv_data:
                        continue

                    output_to_evaluate = conv_data[generated_by_model].get("output", "")
                    if not output_to_evaluate:
                        continue

                    print(f"    è¯„ä¼° {generated_by_model} çš„è¾“å‡º...")

                    # è®©æ¯ä¸ªæ¨¡å‹è¿›è¡Œè¯„ä¼°
                    for evaluator_model in models:
                        total_evaluations += 1

                        # æ£€æŸ¥æ˜¯å¦è·³è¿‡è‡ªæˆ‘è¯„ä¼°
                        if not EVALUATION_CONFIG["include_self_evaluation"] and generated_by_model == evaluator_model:
                            print(f"      è·³è¿‡è‡ªæˆ‘è¯„ä¼°: {evaluator_model}")
                            continue

                        # æ£€æŸ¥ç¼“å­˜ï¼ˆæ£€æŸ¥rawæ–‡ä»¶ï¼‰
                        raw_file = self._get_raw_file_path(
                            patient, conv_id, generated_by_model, evaluator_model
                        )

                        if EVALUATION_CONFIG["enable_caching"] and raw_file.exists():
                            print(f"      âœ“ å·²ç¼“å­˜: {evaluator_model}")
                            successful_evaluations += 1
                            continue

                        # æ‰§è¡Œè¯„ä¼°ï¼ˆä¸¤æ­¥èµ°ï¼šå…ˆä¿å­˜åŸå§‹å“åº”ï¼Œåç»­å†è§£æï¼‰
                        try:
                            # ç¬¬ä¸€æ­¥ï¼šè·å–å¹¶ä¿å­˜åŸå§‹å“åº”
                            raw_response = self._get_raw_evaluation(
                                patient=patient,
                                conversation_id=conv_id,
                                conversation_title=conv_title,
                                generated_by=generated_by_model,
                                evaluated_by=evaluator_model,
                                output_to_evaluate=output_to_evaluate,
                                include_context=include_conversation_context
                            )

                            # ç«‹å³ä¿å­˜åŸå§‹å“åº”
                            self._save_raw_response(
                                patient, conv_id, generated_by_model,
                                evaluator_model, raw_response
                            )

                            print(f"      âœ“ å®Œæˆ: {evaluator_model} (åŸå§‹å“åº”å·²ä¿å­˜)")
                            successful_evaluations += 1

                            # é¿å…APIé™æµ
                            time.sleep(API_CONFIG.get("retry_delay", 1))

                        except Exception as e:
                            print(f"      âœ— å¤±è´¥: {evaluator_model} - {e}")
                            failed_evaluations += 1
                            continue

        results["end_time"] = datetime.now().isoformat()
        results["statistics"] = {
            "total_evaluations": total_evaluations,
            "successful": successful_evaluations,
            "failed": failed_evaluations,
            "success_rate": f"{successful_evaluations/total_evaluations*100:.1f}%" if total_evaluations > 0 else "0%"
        }

        print("\n" + "="*60)
        print("äº¤å‰è¯„ä¼°å®Œæˆ")
        print("="*60)
        print(f"æ€»è¯„ä¼°æ¬¡æ•°: {total_evaluations}")
        print(f"æˆåŠŸ: {successful_evaluations}")
        print(f"å¤±è´¥: {failed_evaluations}")
        print(f"æˆåŠŸç‡: {results['statistics']['success_rate']}")
        print("="*60 + "\n")

        return results

    def _evaluate_output(
        self,
        patient: str,
        conversation_id: str,
        conversation_title: str,
        generated_by: str,
        evaluated_by: str,
        output_to_evaluate: str,
        include_context: bool = True
    ) -> Dict:
        """
        æ‰§è¡Œå•æ¬¡è¯„ä¼°

        Args:
            patient: æ‚£è€…åç§°
            conversation_id: å¯¹è¯ID
            conversation_title: å¯¹è¯æ ‡é¢˜
            generated_by: ç”Ÿæˆè¾“å‡ºçš„æ¨¡å‹
            evaluated_by: è¿›è¡Œè¯„ä¼°çš„æ¨¡å‹
            output_to_evaluate: è¢«è¯„ä¼°çš„è¾“å‡ºå†…å®¹
            include_context: æ˜¯å¦åŒ…å«å¯¹è¯ä¸Šä¸‹æ–‡

        Returns:
            è¯„ä¼°ç»“æœå­—å…¸
        """
        # è·å–å¯¹è¯ä¸Šä¸‹æ–‡
        conversation_context = None
        if include_context:
            conversation_context = self.conversation_indexer.format_conversation_for_evaluation(
                generated_by, patient, conversation_id
            )

        # ç”Ÿæˆè¯„ä¼°prompt
        prompt = self.prompt_template.generate_evaluation_prompt(
            patient=patient,
            conversation_title=conversation_title,
            conversation_id=conversation_id,
            original_output=output_to_evaluate,
            conversation_context=conversation_context
        )

        # ä½¿ç”¨å·¥å‚åˆ›å»ºæ­£ç¡®é…ç½®çš„å®¢æˆ·ç«¯
        try:
            client = self.client_factory.create_client(evaluated_by)
        except ValueError as e:
            print(f"      âš ï¸  æ— æ³•åˆ›å»ºå®¢æˆ·ç«¯: {e}")
            raise

        response = client.chat(
            prompt,
            temperature=API_CONFIG.get("temperature", 0.0),
            max_tokens=8192,
            stream=False  # ä½¿ç”¨éæµå¼æ¨¡å¼ä»¥è·å–å®Œæ•´å“åº”
        )

        # è§£æè¯„ä¼°ç»“æœ
        evaluation_data = self._parse_evaluation_response(response)

        # è®¡ç®—å¹³å‡åˆ†
        scores = [
            dim_data.get("score", 0)
            for dim_data in evaluation_data.get("dimensions", {}).values()
        ]
        average_score = sum(scores) / len(scores) if scores else 0

        # æ„å»ºå®Œæ•´è¯„ä¼°ç»“æœ
        result = {
            "patient": patient,
            "conversation_id": conversation_id,
            "conversation_title": conversation_title,
            "generated_by": generated_by,
            "evaluated_by": evaluated_by,
            "original_output": output_to_evaluate,
            "evaluation": evaluation_data,
            "average_score": round(average_score, 2),
            "metadata": {
                "evaluation_timestamp": datetime.now().isoformat(),
                "evaluator_model_version": evaluated_by,
                "included_conversation_context": include_context
            }
        }

        return result

    def _parse_evaluation_response(self, response: str) -> Dict:
        """
        è§£æè¯„ä¼°å“åº”

        Args:
            response: LLMè¿”å›çš„è¯„ä¼°ç»“æœ

        Returns:
            è§£æåçš„è¯„ä¼°æ•°æ®
        """
        try:
            # å°è¯•ç›´æ¥è§£æJSON
            # ç§»é™¤å¯èƒ½çš„markdownä»£ç å—æ ‡è®°
            cleaned_response = response.strip()
            if cleaned_response.startswith("```"):
                # ç§»é™¤ä»£ç å—æ ‡è®°
                lines = cleaned_response.split('\n')
                cleaned_response = '\n'.join(lines[1:-1])

            evaluation_data = json.loads(cleaned_response)
            return evaluation_data

        except json.JSONDecodeError as e:
            print(f"      âš ï¸  JSONè§£æå¤±è´¥: {e}")
            print(f"      åŸå§‹å“åº”: {response[:200]}...")

            # è¿”å›é»˜è®¤ç»“æ„
            return {
                "dimensions": {
                    "accuracy": {"score": 0, "reasoning": "è§£æå¤±è´¥"},
                    "completeness": {"score": 0, "reasoning": "è§£æå¤±è´¥"},
                    "format": {"score": 0, "reasoning": "è§£æå¤±è´¥"},
                    "language": {"score": 0, "reasoning": "è§£æå¤±è´¥"},
                    "logic": {"score": 0, "reasoning": "è§£æå¤±è´¥"}
                },
                "overall_comment": "è¯„ä¼°å“åº”è§£æå¤±è´¥",
                "strengths": [],
                "weaknesses": [],
                "suggestions": [],
                "parse_error": str(e),
                "raw_response": response
            }

    def _get_conversation_title(self, conv_data: Dict) -> str:
        """ä»å¯¹è¯æ•°æ®ä¸­è·å–æ ‡é¢˜"""
        for model_data in conv_data.values():
            if isinstance(model_data, dict) and "title" in model_data:
                return model_data["title"]
        return "æœªçŸ¥ç±»å‹"

    def _get_evaluation_file_path(
        self,
        patient: str,
        conv_id: str,
        generated_by: str,
        evaluated_by: str
    ) -> Path:
        """è·å–è¯„ä¼°ç»“æœæ–‡ä»¶è·¯å¾„"""
        # åˆ›å»ºæ‚£è€…ç›®å½•
        patient_dir = self.output_base_dir / patient / f"conv_{conv_id}"
        patient_dir.mkdir(parents=True, exist_ok=True)

        # è¯„ä¼°è¯¦æƒ…ç›®å½•
        eval_dir = patient_dir / OUTPUT_CONFIG["evaluations_dir"]
        eval_dir.mkdir(exist_ok=True)

        # æ–‡ä»¶å: {ç”Ÿæˆè€…}_by_{è¯„ä¼°è€…}.json
        filename = f"{generated_by.replace('/', '_')}_by_{evaluated_by.replace('/', '_')}.json"

        return eval_dir / filename

    def _get_raw_file_path(
        self,
        patient: str,
        conv_id: str,
        generated_by: str,
        evaluated_by: str
    ) -> Path:
        """è·å–åŸå§‹å“åº”æ–‡ä»¶è·¯å¾„"""
        # åˆ›å»ºrawå­ç›®å½•
        eval_dir = self.output_base_dir / patient / f"conv_{conv_id}" / "raw"
        eval_dir.mkdir(parents=True, exist_ok=True)

        filename = f"{generated_by.replace('/', '_')}_by_{evaluated_by.replace('/', '_')}.json"
        return eval_dir / filename

    def _save_raw_response(
        self,
        patient: str,
        conv_id: str,
        generated_by: str,
        evaluated_by: str,
        raw_response: Dict
    ):
        """ä¿å­˜åŸå§‹APIå“åº”"""
        file_path = self._get_raw_file_path(
            patient, conv_id, generated_by, evaluated_by
        )

        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(raw_response, f, ensure_ascii=False, indent=2)

    def _get_raw_evaluation(
        self,
        patient: str,
        conversation_id: str,
        conversation_title: str,
        generated_by: str,
        evaluated_by: str,
        output_to_evaluate: str,
        include_context: bool = True
    ) -> Dict:
        """
        è·å–åŸå§‹è¯„ä¼°å“åº”ï¼ˆä¸è§£æï¼‰

        Returns:
            åŒ…å«åŸå§‹å“åº”çš„å­—å…¸
        """
        # è·å–å¯¹è¯ä¸Šä¸‹æ–‡
        conversation_context = None
        if include_context:
            conversation_context = self.conversation_indexer.format_conversation_for_evaluation(
                generated_by, patient, conversation_id
            )

        # ç”Ÿæˆè¯„ä¼°prompt
        prompt = self.prompt_template.generate_evaluation_prompt(
            patient=patient,
            conversation_title=conversation_title,
            conversation_id=conversation_id,
            original_output=output_to_evaluate,
            conversation_context=conversation_context
        )

        # ä½¿ç”¨å·¥å‚åˆ›å»ºæ­£ç¡®é…ç½®çš„å®¢æˆ·ç«¯
        try:
            client = self.client_factory.create_client(evaluated_by)
        except ValueError as e:
            print(f"      âš ï¸  æ— æ³•åˆ›å»ºå®¢æˆ·ç«¯: {e}")
            raise

        # è°ƒç”¨APIè·å–å“åº”
        response = client.chat(
            prompt,
            temperature=API_CONFIG.get("temperature", 0.0),
            max_tokens=8192,
            stream=False
        )

        # æ„å»ºåŸå§‹å“åº”æ•°æ®ï¼ˆä¸è§£æè¯„åˆ†ï¼‰
        raw_data = {
            "patient": patient,
            "conversation_id": conversation_id,
            "conversation_title": conversation_title,
            "generated_by": generated_by,
            "evaluated_by": evaluated_by,
            "original_output": output_to_evaluate,
            "raw_response": response,  # åŸå§‹LLMå“åº”
            "metadata": {
                "timestamp": datetime.now().isoformat(),
                "evaluator_model": evaluated_by,
                "included_context": include_context,
                "prompt_length": len(prompt)
            }
        }

        return raw_data

    def _get_report_raw_file_path(
        self,
        patient: str,
        generated_by: str,
        evaluated_by: str
    ) -> Path:
        """è·å–æŠ¥å‘Šè¯„ä¼°çš„åŸå§‹å“åº”æ–‡ä»¶è·¯å¾„ï¼ˆä¸åˆ†å¯¹è¯ï¼‰"""
        # åˆ›å»ºrawå­ç›®å½•ï¼šæ‚£è€…/raw/
        eval_dir = self.output_base_dir / patient / "raw"
        eval_dir.mkdir(parents=True, exist_ok=True)

        filename = f"{generated_by.replace('/', '_')}_by_{evaluated_by.replace('/', '_')}.json"
        return eval_dir / filename

    def _save_report_raw_response(
        self,
        patient: str,
        generated_by: str,
        evaluated_by: str,
        raw_response: Dict
    ) -> Path:
        """
        ä¿å­˜æŠ¥å‘Šè¯„ä¼°çš„åŸå§‹APIå“åº”

        Returns:
            ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
        """
        try:
            file_path = self._get_report_raw_file_path(
                patient, generated_by, evaluated_by
            )

            # ç¡®ä¿ç›®å½•å­˜åœ¨
            file_path.parent.mkdir(parents=True, exist_ok=True)

            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(raw_response, f, ensure_ascii=False, indent=2)

            return file_path

        except Exception as e:
            print(f"  âœ— ä¿å­˜åŸå§‹å“åº”å¤±è´¥: {e}")
            # å°è¯•ä¿å­˜åˆ°ä¸´æ—¶ä½ç½®
            import tempfile
            temp_file = Path(tempfile.gettempdir()) / f"raw_response_{patient}_{generated_by}_{evaluated_by}.json"
            try:
                with open(temp_file, 'w', encoding='utf-8') as f:
                    json.dump(raw_response, f, ensure_ascii=False, indent=2)
                print(f"  âš ï¸  å·²ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶: {temp_file}")
                return temp_file
            except Exception as temp_error:
                print(f"  âœ— ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶ä¹Ÿå¤±è´¥: {temp_error}")
                raise

    def _get_raw_report_evaluation(
        self,
        patient: str,
        generated_by: str,
        evaluated_by: str,
        report_content: str
    ) -> Dict:
        """
        è·å–å®Œæ•´æŠ¥å‘Šè¯„ä¼°çš„åŸå§‹å“åº”ï¼ˆä¸è§£æï¼‰

        Args:
            patient: æ‚£è€…åç§°
            generated_by: ç”ŸæˆæŠ¥å‘Šçš„æ¨¡å‹
            evaluated_by: è¿›è¡Œè¯„ä¼°çš„æ¨¡å‹
            report_content: å®Œæ•´æŠ¥å‘Šå†…å®¹

        Returns:
            åŒ…å«åŸå§‹å“åº”çš„å­—å…¸
        """
        verbose = EVALUATION_CONFIG.get("verbose_logging", False)

        if verbose:
            tqdm.write(f"\n    ğŸ“ å‡†å¤‡è¯„ä¼°:")
            tqdm.write(f"       ç”Ÿæˆè€…: {generated_by}")
            tqdm.write(f"       è¯„ä¼°è€…: {evaluated_by}")
            tqdm.write(f"       æŠ¥å‘Šé•¿åº¦: {len(report_content)} å­—ç¬¦")

        # ç”Ÿæˆè¯„ä¼°promptï¼ˆé’ˆå¯¹å®Œæ•´æŠ¥å‘Šï¼‰
        try:
            prompt = self.prompt_template.generate_report_evaluation_prompt(
                patient=patient,
                report_content=report_content
            )
        except Exception as e:
            tqdm.write(f"    âœ— ç”ŸæˆPromptå¤±è´¥: {e}")
            raise

        if verbose:
            tqdm.write(f"       Prompté•¿åº¦: {len(prompt)} å­—ç¬¦")
            tqdm.write(f"\n    ğŸ“¥ ã€å…¥å‚ - Prompté¢„è§ˆã€‘")
            tqdm.write(f"    {'-'*70}")
            # æ˜¾ç¤ºpromptçš„å‰500å’Œå200å­—ç¬¦
            if len(prompt) > 700:
                tqdm.write(f"    {prompt[:500]}")
                tqdm.write(f"    ... (çœç•¥ {len(prompt)-700} å­—ç¬¦) ...")
                tqdm.write(f"    {prompt[-200:]}")
            else:
                tqdm.write(f"    {prompt}")
            tqdm.write(f"    {'-'*70}")

        # ä½¿ç”¨å·¥å‚åˆ›å»ºæ­£ç¡®é…ç½®çš„å®¢æˆ·ç«¯
        try:
            client = self.client_factory.create_client(evaluated_by)
        except ValueError as e:
            tqdm.write(f"    âš ï¸  æ— æ³•åˆ›å»ºå®¢æˆ·ç«¯: {e}")
            raise
        except Exception as e:
            tqdm.write(f"    âœ— åˆ›å»ºå®¢æˆ·ç«¯å¤±è´¥: {e}")
            raise

        # è°ƒç”¨APIè·å–å“åº”
        if verbose:
            tqdm.write(f"\n       ğŸ”„ æ­£åœ¨è°ƒç”¨API...")

        start_time = time.time()

        try:
            response = client.chat(
                prompt,
                temperature=API_CONFIG.get("temperature", 0.0),
                max_tokens=8192,
                stream=False
            )
        except Exception as e:
            tqdm.write(f"    âœ— APIè°ƒç”¨å¤±è´¥: {e}")
            raise

        response_time = time.time() - start_time

        if verbose:
            tqdm.write(f"       âœ“ APIå“åº”å®Œæˆ (è€—æ—¶: {response_time:.2f}ç§’)")
            tqdm.write(f"       å“åº”é•¿åº¦: {len(response)} å­—ç¬¦")

            tqdm.write(f"\n    ğŸ“¤ ã€å‡ºå‚ - Responseé¢„è§ˆã€‘")
            tqdm.write(f"    {'-'*70}")
            # æ˜¾ç¤ºresponseçš„å‰800å­—ç¬¦
            if len(response) > 800:
                tqdm.write(f"    {response[:800]}")
                tqdm.write(f"    ... (çœç•¥ {len(response)-800} å­—ç¬¦) ...")
            else:
                tqdm.write(f"    {response}")
            tqdm.write(f"    {'-'*70}\n")

        # æ„å»ºåŸå§‹å“åº”æ•°æ®ï¼ˆåŒ…å«è¾“å…¥å’Œè¾“å‡ºï¼‰
        raw_data = {
            "patient": patient,
            "generated_by": generated_by,
            "evaluated_by": evaluated_by,
            "input": {
                "report_content": report_content,  # å®Œæ•´æŠ¥å‘Šå†…å®¹
                "prompt": prompt,  # å®Œæ•´è¯„ä¼°prompt
                "report_content_length": len(report_content),
                "prompt_length": len(prompt)
            },
            "output": {
                "raw_response": response,  # åŸå§‹LLMå“åº”
                "response_length": len(response),
                "response_time_seconds": response_time
            },
            "metadata": {
                "timestamp": datetime.now().isoformat(),
                "evaluator_model": evaluated_by,
                "evaluation_type": "complete_report",
                "api_config": {
                    "temperature": API_CONFIG.get("temperature", 0.0),
                    "max_tokens": 8192
                }
            }
        }

        return raw_data

    def _get_report_evaluation_file_path(
        self,
        patient: str,
        generated_by: str,
        evaluated_by: str
    ) -> Path:
        """è·å–æŠ¥å‘Šè¯„ä¼°ç»“æœæ–‡ä»¶è·¯å¾„ï¼ˆè§£æåçš„è¯„åˆ†ï¼‰"""
        # åˆ›å»ºevaluationså­ç›®å½•ï¼šæ‚£è€…/evaluations/
        eval_dir = self.output_base_dir / patient / "evaluations"
        eval_dir.mkdir(parents=True, exist_ok=True)

        filename = f"{generated_by.replace('/', '_')}_by_{evaluated_by.replace('/', '_')}.json"
        return eval_dir / filename

    def _parse_and_save_report_evaluation(
        self,
        patient: str,
        generated_by: str,
        evaluated_by: str,
        raw_response: Dict
    ) -> Dict:
        """
        è§£æåŸå§‹å“åº”å¹¶ä¿å­˜è¯„åˆ†ç»“æœ

        Args:
            patient: æ‚£è€…åç§°
            generated_by: ç”ŸæˆæŠ¥å‘Šçš„æ¨¡å‹
            evaluated_by: è¯„ä¼°æ¨¡å‹
            raw_response: åŸå§‹å“åº”æ•°æ®

        Returns:
            è§£æåçš„è¯„ä¼°ç»“æœ
        """
        try:
            # ä»raw_responseä¸­æå–LLMå“åº”æ–‡æœ¬
            llm_response = raw_response.get("output", {}).get("raw_response", "")

            if not llm_response:
                raise ValueError("åŸå§‹å“åº”ä¸ºç©º")

            # è§£æè¯„ä¼°å“åº”
            evaluation_data = self._parse_evaluation_response(llm_response)

            # è®¡ç®—å¹³å‡åˆ†
            scores = [
                dim_data.get("score", 0)
                for dim_data in evaluation_data.get("dimensions", {}).values()
                if isinstance(dim_data, dict) and "score" in dim_data
            ]
            average_score = sum(scores) / len(scores) if scores else 0

            # æ„å»ºå®Œæ•´è¯„ä¼°ç»“æœ
            result = {
                "patient": patient,
                "generated_by": generated_by,
                "evaluated_by": evaluated_by,
                "report_content": raw_response.get("input", {}).get("report_content", ""),
                "evaluation": evaluation_data,
                "average_score": round(average_score, 2),
                "metadata": {
                    "evaluation_timestamp": raw_response.get("metadata", {}).get("timestamp"),
                    "evaluator_model": evaluated_by,
                    "evaluation_type": "complete_report",
                    "response_time_seconds": raw_response.get("output", {}).get("response_time_seconds", 0)
                }
            }

            # ä¿å­˜è¯„ä¼°ç»“æœ
            try:
                file_path = self._get_report_evaluation_file_path(
                    patient, generated_by, evaluated_by
                )

                # ç¡®ä¿ç›®å½•å­˜åœ¨
                file_path.parent.mkdir(parents=True, exist_ok=True)

                with open(file_path, 'w', encoding='utf-8') as f:
                    json.dump(result, f, ensure_ascii=False, indent=2)

            except Exception as save_error:
                print(f"  âš ï¸  ä¿å­˜è¯„ä¼°ç»“æœå¤±è´¥: {save_error}")
                # å³ä½¿ä¿å­˜å¤±è´¥ï¼Œä¹Ÿè¿”å›ç»“æœ
                pass

            return result

        except Exception as e:
            print(f"  âœ— è§£æè¯„ä¼°å“åº”å¤±è´¥: {e}")
            # è¿”å›ä¸€ä¸ªé»˜è®¤çš„ç»“æœ
            return {
                "patient": patient,
                "generated_by": generated_by,
                "evaluated_by": evaluated_by,
                "evaluation": {},
                "average_score": 0,
                "error": str(e),
                "metadata": {
                    "evaluation_timestamp": datetime.now().isoformat(),
                    "evaluator_model": evaluated_by,
                    "evaluation_type": "complete_report",
                    "parse_failed": True
                }
            }

    def _save_evaluation_result(
        self,
        patient: str,
        conv_id: str,
        generated_by: str,
        evaluated_by: str,
        evaluation_result: Dict
    ):
        """ä¿å­˜è¯„ä¼°ç»“æœ"""
        file_path = self._get_evaluation_file_path(
            patient, conv_id, generated_by, evaluated_by
        )

        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(evaluation_result, f, ensure_ascii=False, indent=2)

    def parse_saved_raw_responses(
        self,
        æ‚£è€…åˆ—è¡¨: List[str] = None,
        å¯¹è¯ç±»å‹åˆ—è¡¨: List[str] = None
    ) -> Dict:
        """
        ç¬¬äºŒæ­¥ï¼šè§£æå·²ä¿å­˜çš„åŸå§‹å“åº”

        Args:
            æ‚£è€…åˆ—è¡¨: è¦è§£æçš„æ‚£è€…åˆ—è¡¨
            å¯¹è¯ç±»å‹åˆ—è¡¨: è¦è§£æçš„å¯¹è¯ç±»å‹åˆ—è¡¨

        Returns:
            è§£æç»“æœç»Ÿè®¡
        """
        print("\n" + "="*60)
        print("å¼€å§‹è§£æåŸå§‹å“åº”")
        print("="*60 + "\n")

        patients = æ‚£è€…åˆ—è¡¨ or self.comparison_data.get("patients", [])

        total_parsed = 0
        successful_parsed = 0
        failed_parsed = 0

        for patient in patients:
            print(f"\nå¤„ç†æ‚£è€…: {patient}")
            patient_dir = self.output_base_dir / patient

            if not patient_dir.exists():
                continue

            # éå†æ‰€æœ‰å¯¹è¯ç›®å½•
            for conv_dir in patient_dir.iterdir():
                if not conv_dir.is_dir() or not conv_dir.name.startswith("conv_"):
                    continue

                raw_dir = conv_dir / "raw"
                if not raw_dir.exists():
                    continue

                conv_id = conv_dir.name.replace("conv_", "").split("_")[0]

                # å¦‚æœæŒ‡å®šäº†å¯¹è¯ç±»å‹ï¼Œæ£€æŸ¥æ˜¯å¦åŒ¹é…
                if å¯¹è¯ç±»å‹åˆ—è¡¨ and conv_id not in å¯¹è¯ç±»å‹åˆ—è¡¨:
                    continue

                print(f"  å¯¹è¯ {conv_id}:")

                # è§£ææ‰€æœ‰åŸå§‹å“åº”æ–‡ä»¶
                for raw_file in raw_dir.glob("*.json"):
                    total_parsed += 1

                    try:
                        # è¯»å–åŸå§‹å“åº”
                        with open(raw_file, 'r', encoding='utf-8') as f:
                            raw_data = json.load(f)

                        # è§£æè¯„ä¼°ç»“æœ
                        evaluation_data = self._parse_evaluation_response(
                            raw_data["raw_response"]
                        )

                        # è®¡ç®—å¹³å‡åˆ†
                        scores = [
                            dim_data.get("score", 0)
                            for dim_data in evaluation_data.get("dimensions", {}).values()
                        ]
                        average_score = sum(scores) / len(scores) if scores else 0

                        # æ„å»ºå®Œæ•´è¯„ä¼°ç»“æœ
                        parsed_result = {
                            "patient": raw_data["patient"],
                            "conversation_id": raw_data["conversation_id"],
                            "conversation_title": raw_data["conversation_title"],
                            "generated_by": raw_data["generated_by"],
                            "evaluated_by": raw_data["evaluated_by"],
                            "original_output": raw_data["original_output"],
                            "evaluation": evaluation_data,
                            "average_score": round(average_score, 2),
                            "metadata": raw_data["metadata"]
                        }

                        # ä¿å­˜è§£æåçš„ç»“æœ
                        self._save_evaluation_result(
                            raw_data["patient"],
                            raw_data["conversation_id"],
                            raw_data["generated_by"],
                            raw_data["evaluated_by"],
                            parsed_result
                        )

                        print(f"    âœ“ è§£æ: {raw_file.stem} (å¹³å‡åˆ†: {average_score:.1f})")
                        successful_parsed += 1

                    except Exception as e:
                        print(f"    âœ— è§£æå¤±è´¥: {raw_file.stem} - {e}")
                        failed_parsed += 1

        print("\n" + "="*60)
        print("è§£æå®Œæˆ")
        print("="*60)
        print(f"æ€»æ•°: {total_parsed}")
        print(f"æˆåŠŸ: {successful_parsed}")
        print(f"å¤±è´¥: {failed_parsed}")
        print("="*60 + "\n")

        return {
            "total": total_parsed,
            "successful": successful_parsed,
            "failed": failed_parsed
        }
