# 多模型对比批量处理系统

## 🎯 功能概述

该系统能够**同时使用多个AI模型**处理相同的患者问诊记录，生成详细的对比报告，帮助您：

- ✅ **横向对比**：不同模型对同一Prompt的输出差异
- ✅ **性能对比**：各模型的响应速度、成功率、输出长度
- ✅ **质量评估**：对比输出内容的详细程度和准确性
- ✅ **模型选择**：为您的应用场景选择最合适的模型

## 🚀 快速开始

### 方法1：快速测试（推荐新手）

使用测试版本，只对比2个模型：

```bash
python batch_process_multi_model_test.py
```

**测试配置：**
- 模型：gpt-4o-mini 和 gpt-5.1
- 预计耗时：约1.5-2分钟（2患者 × 5Prompts × 2模型 = 20任务）

### 方法2：完整对比（4个模型）

使用完整版本，对比4个模型：

```bash
python batch_process_multi_model.py
```

**完整配置：**
- 模型：gpt-3.5-turbo, gpt-4o-mini, gpt-4o, gpt-5.1
- 预计耗时：约3-4分钟（2患者 × 5Prompts × 4模型 = 40任务）

## 📊 生成的报告

### 1. 模型对比报告（最重要）

**文件名：** `模型对比报告_时间戳.txt`

**内容：** 横向对比所有模型对同一Prompt的输出

```
====================================================================================================
【主诉】多模型对比
----------------------------------------------------------------------------------------------------

>>> 模型: gpt-4o-mini
>>> 耗时: 4.12秒
>>> 输出长度: 54字符
--------------------------------------------------
主诉：小便泡沫多，持续时间长。糖尿病确诊12年以上，血糖不稳定...

····································································································

>>> 模型: gpt-5.1
>>> 耗时: 3.61秒
>>> 输出长度: 24字符
--------------------------------------------------
发现血糖升高12余年，空腹血糖升高伴体重下降5月

····································································································
```

**用途：**
- 对比不同模型的输出风格
- 评估输出的详细程度
- 选择最符合需求的模型

### 2. 性能对比报告

**文件名：** `性能对比_时间戳.txt`

**内容：** 各模型的性能指标对比

```
模型                   总任务      成功       失败       成功率        总耗时(秒)       平均耗时(秒)      最小耗时(秒)      最大耗时(秒)      平均输出长度
------------------------------------------------------------------------------------------------------------------------------------------------------
gpt-4o-mini          10       10       0        100.0%    44.90        4.49         2.94         6.78         147
gpt-5.1              10       10       0        100.0%    52.39        5.24         2.52         12.17        184
```

**关键指标：**
- **成功率**：模型稳定性
- **平均耗时**：响应速度
- **平均输出长度**：输出详细程度

### 3. 每个模型的独立报告

**目录结构：**
```
模型对比报告/
├── gpt-4o-mini/
│   └── 报告_时间戳.txt
├── gpt-5.1/
│   └── 报告_时间戳.txt
├── gpt-4o/
│   └── 报告_时间戳.txt
└── gpt-3.5-turbo/
    └── 报告_时间戳.txt
```

**用途：**
- 单独查看某个模型的所有输出
- 方便导出特定模型的结果

### 4. 完整JSON报告

**文件名：** `完整报告_时间戳.json`

**内容：** 包含所有详细数据（输入、输出、时间戳等）

**用途：**
- 程序化分析
- 数据导入其他系统
- 深度统计分析

## 📈 测试结果示例

### 性能对比（实测数据）

| 模型 | 平均耗时 | 平均输出长度 | 成功率 |
|------|---------|------------|--------|
| **gpt-4o-mini** | 4.49秒 | 147字符 | 100% |
| **gpt-5.1** | 5.24秒 | 184字符 | 100% |

### 关键发现

1. **响应速度**
   - gpt-4o-mini 平均快 0.75秒（约15%）
   - 最快响应：gpt-4o-mini 2.94秒
   - 最慢响应：gpt-5.1 12.17秒

2. **输出质量**
   - gpt-5.1 输出更详细（平均多37字符，约25%）
   - gpt-5.1 更符合医学术语规范
   - gpt-4o-mini 输出更简洁

3. **稳定性**
   - 两个模型成功率都是100%
   - 无失败任务

## 🔧 自定义配置

### 修改对比的模型

编辑 `batch_process_multi_model.py` 或创建自己的脚本：

```python
# 自定义模型列表
MODELS = [
    "gpt-3.5-turbo",
    "gpt-4o-mini",
    "gpt-4o",
    "gpt-5.1",
    # 添加更多模型...
]

processor = MultiModelBatchProcessor(
    prompts_file="多个Prompt.json",
    records_dir="测试输入问答记录",
    output_dir="我的对比报告",
    models=MODELS  # 使用自定义模型列表
)
```

### 只对比特定Prompt

修改 `多个Prompt.json`，只保留您想测试的Prompt。

### 调整输出目录

```python
OUTPUT_DIR = "我的自定义目录"
```

## 📊 对比分析建议

### 1. 输出质量评估标准

**医学准确性：**
- 是否使用规范的医学术语
- 主诉、现病史等格式是否标准
- 信息是否准确完整

**详细程度：**
- 是否包含所有关键信息
- 时间描述是否精确
- 症状描述是否全面

**简洁性：**
- 是否有冗余信息
- 是否过于简略

### 2. 性能评估标准

**响应速度：**
- 平均耗时 < 5秒：优秀
- 平均耗时 5-10秒：良好
- 平均耗时 > 10秒：需要优化

**稳定性：**
- 成功率 100%：优秀
- 成功率 > 95%：良好
- 成功率 < 95%：需要改进

**输出一致性：**
- 不同患者相同Prompt的输出长度方差小：一致性好

### 3. 性价比评估

考虑因素：
- API调用成本
- 响应速度
- 输出质量
- 稳定性

**推荐策略：**
- 高要求场景：gpt-4o 或 gpt-5.1
- 平衡场景：gpt-4o-mini
- 高吞吐场景：gpt-3.5-turbo

## 🎨 查看报告技巧

### 1. 查看性能对比

```bash
# 快速查看性能表格
cat 模型对比报告_测试/性能对比_*.txt | head -20
```

### 2. 查看特定模型的输出

```bash
# 查看 gpt-5.1 的所有输出
cat 模型对比报告_测试/gpt-5.1/报告_*.txt
```

### 3. 对比特定Prompt

```bash
# 在模型对比报告中搜索"主诉"
grep -A 20 "【主诉】" 模型对比报告_测试/模型对比报告_*.txt
```

### 4. 查看JSON数据

```bash
# 使用jq美化JSON（需要安装jq）
cat 模型对比报告_测试/完整报告_*.json | jq '.[] | .model_stats'
```

## 🔬 深度分析示例

### 分析1：输出长度分布

从JSON报告中提取：

```python
import json

with open('完整报告_xxx.json', 'r') as f:
    data = json.load(f)

for patient in data:
    print(f"\n患者: {patient['patient_name']}")
    for model in patient['results_by_model']:
        lengths = [r['output_length'] for r in patient['results_by_model'][model]]
        print(f"  {model}: 平均长度 {sum(lengths)/len(lengths):.0f}")
```

### 分析2：响应时间分布

```python
for patient in data:
    for model in patient['results_by_model']:
        times = [r['duration_seconds'] for r in patient['results_by_model'][model]]
        print(f"{model}: 平均 {sum(times)/len(times):.2f}秒")
```

## ⚙️ 高级功能

### 1. 添加更多模型

系统支持任何OpenAI兼容的模型，只需添加到列表：

```python
MODELS = [
    "gpt-3.5-turbo",
    "gpt-4o-mini",
    "gpt-4o",
    "gpt-5.1",
    "claude-3-sonnet",  # 如果API支持
    "其他模型名称"
]
```

### 2. 并发控制

如果需要限制并发数（节省资源）：

```python
# 在 process_patient_with_all_models 中添加
semaphore = asyncio.Semaphore(5)  # 最多5个并发任务

async with semaphore:
    # 原有代码...
```

### 3. 重试机制

对于不稳定的模型，添加重试：

```python
for retry in range(3):
    try:
        response = client.chat(...)
        break
    except Exception as e:
        if retry == 2:
            raise
        await asyncio.sleep(2 ** retry)
```

## 📝 日志分析

### 查看执行日志

```bash
tail -f batch_process_multi_model.log
```

### 日志包含信息

- 每个任务的开始/结束时间
- API调用状态
- 错误信息
- 性能统计

### 日志示例

```
2025-11-16 01:43:32 - INFO - [患者1][gpt-4o-mini] 开始处理 主诉 (Prompt 1)
2025-11-16 01:43:36 - INFO - [患者1][gpt-4o-mini] 完成 主诉 (耗时: 4.12秒)
2025-11-16 01:43:36 - INFO - [患者1][gpt-5.1] 开始处理 主诉 (Prompt 1)
2025-11-16 01:43:39 - INFO - [患者1][gpt-5.1] 完成 主诉 (耗时: 3.61秒)
```

## 🎯 使用场景

### 场景1：选择最佳模型

**目标：** 为生产环境选择最合适的模型

**步骤：**
1. 准备10-20个真实患者记录
2. 运行完整对比（4个模型）
3. 分析性能对比报告
4. 人工评估输出质量
5. 综合考虑成本和性能

### 场景2：Prompt优化

**目标：** 优化Prompt以获得更好的输出

**步骤：**
1. 准备多个版本的Prompt
2. 为每个版本运行对比
3. 对比不同Prompt在各模型上的表现
4. 选择最佳Prompt版本

### 场景3：质量监控

**目标：** 持续监控模型输出质量

**步骤：**
1. 定期（如每周）运行对比
2. 记录性能指标变化
3. 发现异常及时调整

## 🐛 常见问题

### Q1: 某个模型一直失败怎么办？

**检查：**
1. 模型名称是否正确
2. API是否支持该模型
3. 查看错误日志

**解决：**
```python
# 临时移除有问题的模型
MODELS = [
    "gpt-4o-mini",  # 保留稳定的模型
    "gpt-5.1"
]
```

### Q2: 处理时间太长怎么办？

**优化方案：**
1. 减少对比的模型数量
2. 减少Prompt数量
3. 减少患者数量
4. 使用更快的模型

### Q3: 如何导出Excel格式？

**方法：**
```python
import pandas as pd
import json

# 读取JSON报告
with open('完整报告_xxx.json', 'r') as f:
    data = json.load(f)

# 转换为DataFrame
df = pd.DataFrame(...)
df.to_excel('对比报告.xlsx')
```

### Q4: 输出长度差异很大正常吗？

**正常情况：**
- gpt-5.1 通常输出更详细（字数多25-50%）
- gpt-3.5-turbo 输出最简洁
- gpt-4o 和 gpt-4o-mini 较平衡

**异常情况：**
- 某个模型输出空白或极短（可能是API问题）
- 输出长度差异超过100%（可能是Prompt理解问题）

## 📚 参考资料

### 相关文件

- `batch_process_multi_model.py` - 主脚本
- `batch_process_multi_model_test.py` - 测试脚本
- `chat_client.py` - API客户端
- `config.py` - 配置文件

### 模型文档

- [OpenAI Models](https://platform.openai.com/docs/models)
- [GPT-4o Mini](https://platform.openai.com/docs/models/gpt-4o-mini)
- [GPT-5.1 Beta](https://platform.openai.com/docs/models/gpt-5)

## 🎉 总结

多模型对比系统帮助您：

✅ **快速对比** 多个模型的输出差异
✅ **量化评估** 性能指标和输出质量
✅ **数据驱动** 做出模型选择决策
✅ **持续优化** 监控和改进系统性能

**下一步：**
1. 运行快速测试验证功能
2. 准备真实数据进行完整对比
3. 分析报告选择最佳模型
4. 部署到生产环境

祝您使用愉快！如有问题请查看日志或联系技术支持。
