#!/usr/bin/env python3
"""
è‡ªåŠ¨é‡è¯•æ‰€æœ‰é›¶åˆ†è¯„ä¼°çš„è„šæœ¬
æ­¥éª¤:
1. è¯†åˆ«é›¶åˆ†è¯„ä¼°
2. åˆ é™¤é›¶åˆ†è¯„ä¼°æ–‡ä»¶
3. è¿è¡Œäº¤å‰è¯„ä¼°é‡æ–°ç”Ÿæˆ
"""

import json
import os
import subprocess
import sys
from pathlib import Path
from datetime import datetime


def identify_zero_scores():
    """è¯†åˆ«é›¶åˆ†è¯„ä¼°"""
    print("=" * 70)
    print("æ­¥éª¤ 1/3: è¯†åˆ«é›¶åˆ†è¯„ä¼°")
    print("=" * 70)

    result = subprocess.run(
        [sys.executable, "identify_failed_evals.py"],
        capture_output=True,
        text=True
    )

    print(result.stdout)
    if result.returncode != 0:
        print("âŒ è¯†åˆ«å¤±è´¥:")
        print(result.stderr)
        return False

    # æ£€æŸ¥æ˜¯å¦ç”Ÿæˆäº†é‡è¯•åˆ—è¡¨
    if not Path("retry_zero_scores.json").exists():
        print("âŒ æœªç”Ÿæˆé‡è¯•åˆ—è¡¨æ–‡ä»¶")
        return False

    with open("retry_zero_scores.json", 'r', encoding='utf-8') as f:
        zero_evals = json.load(f)

    return len(zero_evals)


def delete_zero_score_files():
    """åˆ é™¤é›¶åˆ†è¯„ä¼°æ–‡ä»¶"""
    print()
    print("=" * 70)
    print("æ­¥éª¤ 2/3: åˆ é™¤é›¶åˆ†è¯„ä¼°æ–‡ä»¶")
    print("=" * 70)

    with open("retry_zero_scores.json", 'r', encoding='utf-8') as f:
        zero_evals = json.load(f)

    deleted_count = 0
    for item in zero_evals:
        file_path = Path(item['file'])
        if file_path.exists():
            try:
                os.remove(file_path)
                deleted_count += 1
                print(f"  âœ“ å·²åˆ é™¤: {item['patient']} - {item['model']} â†’ {item['evaluator']}")
            except Exception as e:
                print(f"  âœ— åˆ é™¤å¤±è´¥: {file_path} - {e}")

    print(f"\nâœ… å·²åˆ é™¤ {deleted_count} ä¸ªé›¶åˆ†è¯„ä¼°æ–‡ä»¶")
    return deleted_count


def run_cross_evaluation():
    """è¿è¡Œäº¤å‰è¯„ä¼°"""
    print()
    print("=" * 70)
    print("æ­¥éª¤ 3/3: è¿è¡Œäº¤å‰è¯„ä¼° (ä»…é‡è¯•åˆ é™¤çš„æ–‡ä»¶)")
    print("=" * 70)
    print()

    # ç”Ÿæˆæ—¥å¿—æ–‡ä»¶å
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_file = f"retry_zero_scores_{timestamp}.log"

    print(f"ğŸ“ æ—¥å¿—æ–‡ä»¶: {log_file}")
    print()
    print("â³ å¼€å§‹è¯„ä¼°... (è¿™å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´)")
    print()

    # è¿è¡Œè¯„ä¼°å¹¶å®æ—¶æ˜¾ç¤ºè¾“å‡º
    with open(log_file, 'w', encoding='utf-8') as log_f:
        process = subprocess.Popen(
            [sys.executable, "run_cross_evaluation.py"],
            stdout=subprocess.PIPE,
            stderr=subprocess.STDOUT,
            text=True,
            bufsize=1,
            universal_newlines=True
        )

        # å®æ—¶æ˜¾ç¤ºè¾“å‡ºå¹¶å†™å…¥æ—¥å¿—
        for line in process.stdout:
            print(line, end='')
            log_f.write(line)

        process.wait()

        if process.returncode == 0:
            print()
            print("âœ… äº¤å‰è¯„ä¼°å®Œæˆ")
            return True
        else:
            print()
            print(f"âŒ äº¤å‰è¯„ä¼°å¤±è´¥ (è¿”å›ç : {process.returncode})")
            return False


def verify_results():
    """éªŒè¯ç»“æœ"""
    print()
    print("=" * 70)
    print("éªŒè¯ç»“æœ")
    print("=" * 70)

    base_dir = Path("output/cross_evaluation_results")
    total_evals = 0
    zero_scores = 0

    for patient_dir in sorted(base_dir.glob("æ‚£è€…*")):
        eval_dir = patient_dir / "evaluations"
        if not eval_dir.exists():
            continue

        for eval_file in eval_dir.glob("*.json"):
            try:
                with open(eval_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                total_evals += 1
                if data.get('average_score', -1) == 0.0:
                    zero_scores += 1
            except:
                pass

    print(f"æ€»è¯„ä¼°æ•°: {total_evals}")
    print(f"é›¶åˆ†è¯„ä¼°: {zero_scores}")
    print(f"æœ‰æ•ˆè¯„ä¼°: {total_evals - zero_scores}")
    print()

    if zero_scores == 0:
        print("ğŸ‰ğŸ‰ğŸ‰ å®Œç¾! æ‰€æœ‰è¯„ä¼°éƒ½æœ‰æœ‰æ•ˆåˆ†æ•°! ğŸ‰ğŸ‰ğŸ‰")
        return True
    else:
        print(f"âš ï¸  ä»æœ‰ {zero_scores} ä¸ªé›¶åˆ†è¯„ä¼°")
        return False


def main():
    print()
    print("â•”" + "â•" * 68 + "â•—")
    print("â•‘" + " " * 20 + "é›¶åˆ†è¯„ä¼°è‡ªåŠ¨é‡è¯•ç³»ç»Ÿ" + " " * 20 + "â•‘")
    print("â•š" + "â•" * 68 + "â•")
    print()

    # æ­¥éª¤1: è¯†åˆ«
    zero_count = identify_zero_scores()
    if not zero_count:
        print("âŒ è¯†åˆ«å¤±è´¥ï¼Œé€€å‡º")
        return

    if zero_count == 0:
        print("âœ… æ²¡æœ‰é›¶åˆ†è¯„ä¼°ï¼Œæ— éœ€é‡è¯•")
        return

    print()
    print(f"å‘ç° {zero_count} ä¸ªé›¶åˆ†è¯„ä¼°éœ€è¦é‡è¯•")
    print()

    # æ­¥éª¤2: åˆ é™¤
    deleted = delete_zero_score_files()
    if deleted == 0:
        print("âŒ æœªåˆ é™¤ä»»ä½•æ–‡ä»¶ï¼Œé€€å‡º")
        return

    # æ­¥éª¤3: é‡æ–°è¯„ä¼°
    success = run_cross_evaluation()
    if not success:
        print()
        print("âŒ é‡è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ—¥å¿—")
        return

    # æ­¥éª¤4: éªŒè¯
    all_valid = verify_results()

    print()
    print("=" * 70)
    if all_valid:
        print("âœ¨ é‡è¯•å®Œæˆ! æ‰€æœ‰640ä¸ªè¯„ä¼°éƒ½æœ‰æœ‰æ•ˆåˆ†æ•°!")
    else:
        print("âš ï¸  é‡è¯•å®Œæˆï¼Œä½†ä»æœ‰éƒ¨åˆ†è¯„ä¼°ä¸ºé›¶åˆ†")
        print("   å¯èƒ½éœ€è¦å†æ¬¡è¿è¡Œæ­¤è„šæœ¬")
    print("=" * 70)
    print()


if __name__ == "__main__":
    main()
