#!/usr/bin/env python3
import json
from pathlib import Path
from collections import defaultdict

results_dir = Path("output/cross_evaluation_results")

# ç»Ÿè®¡å„æ¨¡å‹ä½œä¸ºè¢«è¯„æµ‹è€…çš„åˆ†æ•°
evaluated_stats = defaultdict(lambda: {"scores": [], "by_evaluator": defaultdict(list)})

# ç»Ÿè®¡å„æ¨¡å‹ä½œä¸ºè¯„æµ‹è€…çš„è¯„åˆ†æƒ…å†µ
evaluator_stats = defaultdict(lambda: {"given_scores": [], "count": 0})

for patient_dir in sorted(results_dir.glob("æ‚£è€…*")):
    for agg_file in patient_dir.glob("*_aggregated.json"):
        parts = agg_file.stem.split("_by_")
        evaluated = parts[0]
        evaluator = parts[1].replace(f"_{patient_dir.name}_aggregated", "")
        
        with open(agg_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
            total_score = data.get("æ€»åˆ†") or data.get("total_score", 0)
            if total_score:
                evaluated_stats[evaluated]["scores"].append(total_score)
                evaluated_stats[evaluated]["by_evaluator"][evaluator].append(total_score)
                evaluator_stats[evaluator]["given_scores"].append(total_score)
                evaluator_stats[evaluator]["count"] += 1

print("=" * 90)
print("ğŸ“Š äº¤å‰è¯„æµ‹å®Œæ•´æŠ¥å‘Š")
print("=" * 90)

print("\nã€ä¸€ã€‘å„æ¨¡å‹è¢«è¯„æµ‹è¡¨ç°ï¼ˆä½œä¸ºè¢«æµ‹æ¨¡å‹ï¼‰")
print("-" * 90)
print(f"{'æ¨¡å‹åç§°':30s} | {'å®Œæˆç‡':8s} | {'å¹³å‡åˆ†':6s} | {'æœ€ä½åˆ†':6s} | {'æœ€é«˜åˆ†':6s} | {'æ ‡å‡†å·®':6s}")
print("-" * 90)

import statistics
for model in sorted(evaluated_stats.keys()):
    scores = evaluated_stats[model]["scores"]
    if scores:
        avg = sum(scores) / len(scores)
        std = statistics.stdev(scores) if len(scores) > 1 else 0
        print(f"{model:30s} | {len(scores):3d}/80  | {avg:6.2f} | {min(scores):6d} | {max(scores):6d} | {std:6.2f}")

print("\nã€äºŒã€‘å„æ¨¡å‹è¯„æµ‹è¡Œä¸ºï¼ˆä½œä¸ºè¯„æµ‹è€…ï¼‰")
print("-" * 90)
print(f"{'è¯„æµ‹è€…æ¨¡å‹':30s} | {'è¯„æµ‹æ•°':8s} | {'å¹³å‡ç»™åˆ†':8s} | {'è¯„åˆ†èŒƒå›´':15s} | {'æ ‡å‡†å·®':6s}")
print("-" * 90)

for evaluator in sorted(evaluator_stats.keys()):
    given = evaluator_stats[evaluator]["given_scores"]
    count = evaluator_stats[evaluator]["count"]
    if given:
        avg = sum(given) / len(given)
        std = statistics.stdev(given) if len(given) > 1 else 0
        score_range = f"{min(given)}-{max(given)}"
        print(f"{evaluator:30s} | {count:3d}/80  | {avg:8.2f} | {score_range:15s} | {std:6.2f}")

print("\nã€ä¸‰ã€‘æ¨¡å‹äº’è¯„çŸ©é˜µï¼ˆå¹³å‡åˆ†ï¼‰")
print("-" * 90)

all_models = sorted(evaluated_stats.keys())
# Print header
print(f"{'è¢«è¯„æµ‹â†“ / è¯„æµ‹è€…â†’':25s}", end="")
for evaluator in all_models[:4]:
    print(f" | {evaluator[:8]:8s}", end="")
print()
print("-" * 90)

for evaluated in all_models:
    print(f"{evaluated[:25]:25s}", end="")
    for evaluator in all_models[:4]:
        scores = evaluated_stats[evaluated]["by_evaluator"].get(evaluator, [])
        if scores:
            avg = sum(scores) / len(scores)
            print(f" | {avg:8.1f}", end="")
        else:
            print(f" | {'---':8s}", end="")
    print()

print("\nã€å››ã€‘æ€»ä½“ç»Ÿè®¡")
print("-" * 90)
total_completed = sum(len(s['scores']) for s in evaluated_stats.values())
total_expected = 640
print(f"âœ“ å®Œæˆè¯„æµ‹: {total_completed}/{total_expected} ({total_completed/total_expected*100:.1f}%)")
print(f"âœ— å¤±è´¥ä»»åŠ¡: {total_expected - total_completed}")
print(f"ğŸ“ ç”Ÿæˆæ–‡ä»¶: {len(list(results_dir.glob('**/*.json')))} ä¸ªJSONæ–‡ä»¶")

all_scores = []
for model_stats in evaluated_stats.values():
    all_scores.extend(model_stats["scores"])
if all_scores:
    print(f"ğŸ“ˆ æ‰€æœ‰è¯„åˆ†ç»Ÿè®¡: å¹³å‡ {sum(all_scores)/len(all_scores):.2f}, ä¸­ä½æ•° {statistics.median(all_scores):.2f}")
