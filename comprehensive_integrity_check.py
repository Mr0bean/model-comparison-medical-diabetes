#!/usr/bin/env python3
"""å…¨é¢çš„å®Œæ•´æ€§æ£€æŸ¥"""

import json
from pathlib import Path
from collections import defaultdict
import statistics

# å®šä¹‰æ¨¡å‹åˆ—è¡¨
MODELS = [
    "Baichuan-M2",
    "deepseek_deepseek-v3.1",
    "doubao-seed-1-6-251015",
    "gemini-2.5-pro",
    "gpt-5.1",
    "grok-4-0709",
    "moonshotai_kimi-k2-0905",
    "qwen3-max"
]

PATIENTS = [f"æ‚£è€…{i}" for i in range(1, 11)]

base_dir = Path("output/cross_evaluation_results")

print("=" * 80)
print("å…¨é¢å®Œæ•´æ€§æ£€æŸ¥")
print("=" * 80)
print()

# æ£€æŸ¥1: æ–‡ä»¶æ•°é‡
print("ã€æ£€æŸ¥1ã€‘æ–‡ä»¶æ•°é‡éªŒè¯")
print("-" * 80)

total_expected = len(PATIENTS) * len(MODELS) * len(MODELS)
print(f"é¢„æœŸæ€»æ•°: {total_expected} (10æ‚£è€… Ã— 8æ¨¡å‹ Ã— 8è¯„ä¼°è€…)")

total_found = 0
total_valid = 0
total_zero = 0

patient_stats = {}

for patient in PATIENTS:
    eval_dir = base_dir / patient / "evaluations"

    if not eval_dir.exists():
        print(f"âŒ {patient}: ç›®å½•ä¸å­˜åœ¨!")
        patient_stats[patient] = {"found": 0, "valid": 0, "zero": 0}
        continue

    found = len(list(eval_dir.glob("*.json")))
    valid = 0
    zero = 0

    for eval_file in eval_dir.glob("*.json"):
        try:
            with open(eval_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            score = data.get('average_score', -1)
            if score > 0:
                valid += 1
            elif score == 0:
                zero += 1
        except:
            pass

    total_found += found
    total_valid += valid
    total_zero += zero

    patient_stats[patient] = {"found": found, "valid": valid, "zero": zero}

    status = "âœ“" if found == 64 and valid == 64 else "âœ—"
    print(f"{status} {patient}: {found}/64 æ–‡ä»¶, {valid} æœ‰æ•ˆ, {zero} é›¶åˆ†")

print()
print(f"æ€»è®¡: {total_found}/{total_expected} æ–‡ä»¶")
print(f"      {total_valid} æœ‰æ•ˆè¯„ä¼°")
print(f"      {total_zero} é›¶åˆ†è¯„ä¼°")
print()

# æ£€æŸ¥2: ç¼ºå¤±çš„è¯„ä¼°ç»„åˆ
print("ã€æ£€æŸ¥2ã€‘ç¼ºå¤±è¯„ä¼°æ£€æŸ¥")
print("-" * 80)

all_missing = []

for patient in PATIENTS:
    eval_dir = base_dir / patient / "evaluations"

    if not eval_dir.exists():
        continue

    patient_missing = []

    for gen_model in MODELS:
        for eval_model in MODELS:
            filename = f"{gen_model}_by_{eval_model}.json"
            filepath = eval_dir / filename

            if not filepath.exists():
                patient_missing.append(f"{gen_model} â†’ {eval_model}")
                all_missing.append({
                    "patient": patient,
                    "generated_by": gen_model,
                    "evaluated_by": eval_model
                })

    if patient_missing:
        print(f"{patient}: ç¼ºå¤± {len(patient_missing)} ä¸ª")
        for m in patient_missing[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
            print(f"  - {m}")
        if len(patient_missing) > 5:
            print(f"  ... è¿˜æœ‰ {len(patient_missing) - 5} ä¸ª")

if not all_missing:
    print("âœ“ æ²¡æœ‰ç¼ºå¤±çš„è¯„ä¼°ç»„åˆ!")
else:
    print(f"âŒ æ€»è®¡ç¼ºå¤±: {len(all_missing)} ä¸ªè¯„ä¼°")

print()

# æ£€æŸ¥3: é›¶åˆ†è¯„ä¼°è¯¦æƒ…
print("ã€æ£€æŸ¥3ã€‘é›¶åˆ†è¯„ä¼°è¯¦æƒ…")
print("-" * 80)

zero_score_details = []

for patient in PATIENTS:
    eval_dir = base_dir / patient / "evaluations"

    if not eval_dir.exists():
        continue

    for eval_file in eval_dir.glob("*.json"):
        try:
            with open(eval_file, 'r', encoding='utf-8') as f:
                data = json.load(f)

            score = data.get('average_score', -1)

            if score == 0:
                filename = eval_file.stem
                parts = filename.split('_by_')
                if len(parts) == 2:
                    zero_score_details.append({
                        "patient": patient,
                        "model": parts[0],
                        "evaluator": parts[1],
                        "file": str(eval_file)
                    })
        except:
            pass

if zero_score_details:
    print(f"âŒ å‘ç° {len(zero_score_details)} ä¸ªé›¶åˆ†è¯„ä¼°:")
    for item in zero_score_details[:10]:
        print(f"  - {item['patient']}: {item['model']} â†’ {item['evaluator']}")
    if len(zero_score_details) > 10:
        print(f"  ... è¿˜æœ‰ {len(zero_score_details) - 10} ä¸ª")
else:
    print("âœ“ æ²¡æœ‰é›¶åˆ†è¯„ä¼°!")

print()

# æ£€æŸ¥4: åˆ†æ•°ç»Ÿè®¡åˆ†æ
print("ã€æ£€æŸ¥4ã€‘åˆ†æ•°ç»Ÿè®¡åˆ†æ")
print("-" * 80)

all_scores = []
model_scores = defaultdict(list)
evaluator_scores = defaultdict(list)

for patient in PATIENTS:
    eval_dir = base_dir / patient / "evaluations"

    if not eval_dir.exists():
        continue

    for eval_file in eval_dir.glob("*.json"):
        try:
            with open(eval_file, 'r', encoding='utf-8') as f:
                data = json.load(f)

            score = data.get('average_score', 0)

            if score > 0:
                all_scores.append(score)

                filename = eval_file.stem
                parts = filename.split('_by_')
                if len(parts) == 2:
                    model_scores[parts[0]].append(score)
                    evaluator_scores[parts[1]].append(score)
        except:
            pass

if all_scores:
    print(f"æœ‰æ•ˆè¯„ä¼°æ•°: {len(all_scores)}")
    print(f"å¹³å‡åˆ†: {statistics.mean(all_scores):.2f}")
    print(f"æ ‡å‡†å·®: {statistics.stdev(all_scores):.2f}")
    print(f"ä¸­ä½æ•°: {statistics.median(all_scores):.2f}")
    print(f"æœ€é«˜åˆ†: {max(all_scores):.2f}")
    print(f"æœ€ä½åˆ†: {min(all_scores):.2f}")
    print()

    # åˆ†æ•°åˆ†å¸ƒ
    ranges = {
        "1.0-2.0": 0,
        "2.0-3.0": 0,
        "3.0-4.0": 0,
        "4.0-5.0": 0
    }

    for score in all_scores:
        if 1.0 <= score < 2.0:
            ranges["1.0-2.0"] += 1
        elif 2.0 <= score < 3.0:
            ranges["2.0-3.0"] += 1
        elif 3.0 <= score < 4.0:
            ranges["3.0-4.0"] += 1
        elif 4.0 <= score <= 5.0:
            ranges["4.0-5.0"] += 1

    print("åˆ†æ•°åˆ†å¸ƒ:")
    for range_name, count in ranges.items():
        pct = count / len(all_scores) * 100
        bar = "â–ˆ" * int(pct / 2)
        print(f"  {range_name}: {count:3d} ({pct:5.1f}%) {bar}")

print()

# æ£€æŸ¥5: æ¯ä¸ªæ¨¡å‹çš„è¯„ä¼°è¦†ç›–åº¦
print("ã€æ£€æŸ¥5ã€‘æ¨¡å‹è¯„ä¼°è¦†ç›–åº¦")
print("-" * 80)

for model in MODELS:
    # ä½œä¸ºè¢«è¯„ä¼°è€…
    as_subject_count = len(model_scores.get(model, []))
    expected_as_subject = len(PATIENTS) * len(MODELS)  # 10æ‚£è€… Ã— 8è¯„ä¼°è€…

    # ä½œä¸ºè¯„ä¼°è€…
    as_evaluator_count = len(evaluator_scores.get(model, []))
    expected_as_evaluator = len(PATIENTS) * len(MODELS)  # 10æ‚£è€… Ã— 8è¢«è¯„ä¼°è€…

    subject_status = "âœ“" if as_subject_count == expected_as_subject else "âœ—"
    evaluator_status = "âœ“" if as_evaluator_count == expected_as_evaluator else "âœ—"

    print(f"{model}:")
    print(f"  {subject_status} ä½œä¸ºè¢«è¯„ä¼°è€…: {as_subject_count}/{expected_as_subject}")
    print(f"  {evaluator_status} ä½œä¸ºè¯„ä¼°è€…: {as_evaluator_count}/{expected_as_evaluator}")

print()

# æœ€ç»ˆç»“è®º
print("=" * 80)
print("ã€æœ€ç»ˆç»“è®ºã€‘")
print("=" * 80)

issues = []

if total_found != total_expected:
    issues.append(f"æ–‡ä»¶æ•°é‡ä¸å®Œæ•´: {total_found}/{total_expected}")

if total_zero > 0:
    issues.append(f"å­˜åœ¨é›¶åˆ†è¯„ä¼°: {total_zero}ä¸ª")

if all_missing:
    issues.append(f"ç¼ºå¤±è¯„ä¼°ç»„åˆ: {len(all_missing)}ä¸ª")

if issues:
    print("âŒ å‘ç°é—®é¢˜:")
    for issue in issues:
        print(f"  - {issue}")
else:
    print("âœ… å®Œæ•´æ€§æ£€æŸ¥é€šè¿‡!")
    print(f"   - æ‰€æœ‰ {total_expected} ä¸ªè¯„ä¼°æ–‡ä»¶å­˜åœ¨")
    print(f"   - æ‰€æœ‰è¯„ä¼°éƒ½æœ‰æœ‰æ•ˆåˆ†æ•°")
    print(f"   - æ²¡æœ‰ç¼ºå¤±çš„è¯„ä¼°ç»„åˆ")
    print()
    print("ğŸ‰ğŸ‰ğŸ‰ æ•°æ®å®Œæ•´! å¯ä»¥è¿›è¡Œä¸‹ä¸€æ­¥åˆ†æ! ğŸ‰ğŸ‰ğŸ‰")

print("=" * 80)
