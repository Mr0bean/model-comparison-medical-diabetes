#!/usr/bin/env python3
"""
å®Œæ•´æŠ¥å‘Šäº¤å‰è¯„ä¼° - ä¸»ç¨‹åº
è¯„ä¼° output/raw/ ä¸­å·²ç”Ÿæˆçš„æ¨¡å‹æŠ¥å‘Š
"""

import argparse
import logging
from pathlib import Path

from cross_evaluation.report_loader import ReportLoader
from cross_evaluation.report_evaluation_engine import ReportEvaluationEngine
from cross_evaluation.model_registry import ModelRegistry


def setup_logging(log_level: str = "INFO"):
    """é…ç½®æ—¥å¿—"""
    logging.basicConfig(
        level=getattr(logging, log_level.upper()),
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="å®Œæ•´æŠ¥å‘Šäº¤å‰è¯„ä¼°ç³»ç»Ÿ")

    parser.add_argument(
        '--models',
        nargs='+',
        help='è¦è¯„ä¼°çš„æ¨¡å‹åˆ—è¡¨ï¼ˆé»˜è®¤ä½¿ç”¨æ‰€æœ‰å¯ç”¨æ¨¡å‹ï¼‰'
    )

    parser.add_argument(
        '--patients',
        nargs='+',
        help='è¦è¯„ä¼°çš„æ‚£è€…åˆ—è¡¨ï¼ˆé»˜è®¤è¯„ä¼°æ‰€æœ‰æ‚£è€…ï¼‰'
    )

    parser.add_argument(
        '--reports-dir',
        default='output/raw',
        help='æŠ¥å‘Šç›®å½•è·¯å¾„ï¼ˆé»˜è®¤: output/rawï¼‰'
    )

    parser.add_argument(
        '--output-dir',
        default='output/report_cross_evaluation',
        help='è¾“å‡ºç›®å½•è·¯å¾„ï¼ˆé»˜è®¤: output/report_cross_evaluationï¼‰'
    )

    parser.add_argument(
        '--test-mode',
        action='store_true',
        help='æµ‹è¯•æ¨¡å¼ï¼šåªè¯„ä¼°ç¬¬ä¸€ä¸ªæ‚£è€…'
    )

    parser.add_argument(
        '--include-self-evaluation',
        action='store_true',
        help='åŒ…å«æ¨¡å‹è‡ªæˆ‘è¯„ä¼°'
    )

    parser.add_argument(
        '--log-level',
        default='INFO',
        choices=['DEBUG', 'INFO', 'WARNING', 'ERROR'],
        help='æ—¥å¿—çº§åˆ«'
    )

    parser.add_argument(
        '--yes', '-y',
        action='store_true',
        help='è·³è¿‡ç¡®è®¤æç¤ºï¼Œè‡ªåŠ¨ç»§ç»­'
    )

    args = parser.parse_args()

    # é…ç½®æ—¥å¿—
    setup_logging(args.log_level)

    print("\n" + "="*80)
    print("ğŸ¤– å®Œæ•´æŠ¥å‘Šäº¤å‰è¯„ä¼°ç³»ç»Ÿ")
    print("="*80 + "\n")

    # åˆå§‹åŒ–
    report_loader = ReportLoader(args.reports_dir)
    engine = ReportEvaluationEngine(
        reports_dir=args.reports_dir,
        output_dir=args.output_dir
    )

    # è·å–å¯ç”¨æŠ¥å‘Š
    available_reports = report_loader.get_available_reports()

    if not available_reports:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„æŠ¥å‘Š")
        print(f"   è¯·æ£€æŸ¥ç›®å½•: {args.reports_dir}")
        return

    print(f"ğŸ“Š å‘ç°æŠ¥å‘Š:")
    print(f"   æ‚£è€…æ•°é‡: {len(available_reports)}")

    # ç¡®å®šè¦è¯„ä¼°çš„æ‚£è€…
    if args.patients:
        patients = [p for p in args.patients if p in available_reports]
        if not patients:
            print(f"âŒ æŒ‡å®šçš„æ‚£è€…æ²¡æœ‰æŠ¥å‘Š: {args.patients}")
            return
    else:
        patients = sorted(available_reports.keys())

    if args.test_mode:
        patients = patients[:1]
        print(f"\nâš ï¸  æµ‹è¯•æ¨¡å¼ï¼šåªè¯„ä¼°ç¬¬ä¸€ä¸ªæ‚£è€… ({patients[0]})")

    print(f"\nğŸ¯ è¯„ä¼°èŒƒå›´:")
    print(f"   æ‚£è€…: {', '.join(patients)}")

    # ç¡®å®šè¦ä½¿ç”¨çš„æ¨¡å‹
    if args.models:
        models = args.models
    else:
        # ä½¿ç”¨æ‰€æœ‰å·²æ³¨å†Œçš„æ¨¡å‹
        registry = ModelRegistry()
        models = list(registry.list_models().keys())

    print(f"   æ¨¡å‹: {', '.join(models)}")
    print(f"   çŸ©é˜µå¤§å°: {len(models)} Ã— {len(models)}")
    print(f"   è‡ªæˆ‘è¯„ä¼°: {'æ˜¯' if args.include_self_evaluation else 'å¦'}")

    # è®¡ç®—è¯„ä¼°æ¬¡æ•°
    evals_per_patient = len(models) * len(models)
    if not args.include_self_evaluation:
        evals_per_patient -= len(models)

    total_evals = len(patients) * evals_per_patient
    print(f"   é¢„è®¡è¯„ä¼°æ¬¡æ•°: {total_evals} ({len(patients)}æ‚£è€… Ã— {evals_per_patient}è¯„ä¼°)")

    # ç¡®è®¤
    print(f"\nè¾“å‡ºç›®å½•: {args.output_dir}")
    if not args.yes:
        choice = input("\næ˜¯å¦ç»§ç»­ï¼Ÿ(Y/n): ")
        if choice.lower() == 'n':
            print("å·²å–æ¶ˆ")
            return
    else:
        print("\nè‡ªåŠ¨ç¡®è®¤ï¼Œå¼€å§‹è¯„ä¼°...")

    # å¼€å§‹è¯„ä¼°
    print("\n" + "="*80)
    print("å¼€å§‹äº¤å‰è¯„ä¼°")
    print("="*80 + "\n")

    results = []
    for i, patient in enumerate(patients, 1):
        print(f"\n[{i}/{len(patients)}] è¯„ä¼°æ‚£è€…: {patient}")

        result = engine.evaluate_patient_reports(
            patient=patient,
            models=models,
            include_self_evaluation=args.include_self_evaluation
        )

        results.append(result)

    # ç”Ÿæˆæ±‡æ€»ç»Ÿè®¡
    print("\n" + "="*80)
    print("ğŸ“Š æ±‡æ€»ç»Ÿè®¡")
    print("="*80 + "\n")

    # æŒ‰æ¨¡å‹æ±‡æ€»å¹³å‡åˆ†
    model_scores = {}
    for result in results:
        for model in result['models']:
            if model not in model_scores:
                model_scores[model] = []

            # è·å–è¯¥æ¨¡å‹åœ¨è¿™ä¸ªæ‚£è€…ä¸­çš„å¹³å‡åˆ†
            patient_score = result['matrix']['statistics']['model_average_scores'].get(model, 0)
            model_scores[model].append(patient_score)

    # è®¡ç®—æ€»ä½“å¹³å‡åˆ†
    print("ğŸ† æ¨¡å‹æ€»ä½“æ’åï¼ˆè·¨æ‚£è€…å¹³å‡ï¼‰:")
    print("-" * 60)

    import numpy as np

    overall_rankings = []
    for model, scores in model_scores.items():
        mean_score = np.mean(scores)
        std_score = np.std(scores)
        overall_rankings.append({
            'model': model,
            'mean': round(mean_score, 2),
            'std': round(std_score, 2),
            'count': len(scores)
        })

    overall_rankings.sort(key=lambda x: x['mean'], reverse=True)

    for i, ranking in enumerate(overall_rankings, 1):
        print(f"   {i}. {ranking['model']:<30} - å¹³å‡åˆ†: {ranking['mean']:.2f} (Â±{ranking['std']:.2f}, n={ranking['count']})")

    # ä¿å­˜æ±‡æ€»ç»Ÿè®¡
    summary_dir = Path(args.output_dir) / "summary"
    summary_dir.mkdir(parents=True, exist_ok=True)

    summary_file = summary_dir / "overall_statistics.json"
    import json
    with open(summary_file, 'w', encoding='utf-8') as f:
        json.dump({
            'patients': patients,
            'models': models,
            'overall_rankings': overall_rankings,
            'patient_results': [
                {
                    'patient': r['patient'],
                    'statistics': r['statistics'],
                    'rankings': r['matrix']['statistics']['model_rankings']
                }
                for r in results
            ]
        }, f, ensure_ascii=False, indent=2)

    print(f"\nâœ“ æ±‡æ€»ç»Ÿè®¡å·²ä¿å­˜: {summary_file}")

    print("\n" + "="*80)
    print("âœ¨ äº¤å‰è¯„ä¼°å®Œæˆï¼")
    print("="*80)

    print(f"\nğŸ“ ç»“æœä¿å­˜ä½ç½®: {args.output_dir}/")
    print(f"\nğŸ“Š æŸ¥çœ‹ç»“æœ:")
    print(f"   - æ‚£è€…è¯¦ç»†ç»“æœ: {{output_dir}}/{{æ‚£è€…}}/complete_result.json")
    print(f"   - å•ä¸ªè¯„ä¼°: {{output_dir}}/{{æ‚£è€…}}/evaluations/{{æ¨¡å‹}}_evaluated_by_{{æ¨¡å‹}}.json")
    print(f"   - æ±‡æ€»ç»Ÿè®¡: {{output_dir}}/summary/overall_statistics.json")
    print("="*80 + "\n")


if __name__ == "__main__":
    main()
