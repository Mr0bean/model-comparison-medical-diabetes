#!/usr/bin/env python3
"""è¯†åˆ«æ‰€æœ‰éœ€è¦é‡è¯•çš„é›¶åˆ†è¯„ä¼°"""

import json
import os
from pathlib import Path
from collections import defaultdict

def identify_zero_score_evaluations():
    """è¯†åˆ«æ‰€æœ‰average_score=0.0çš„è¯„ä¼°"""
    base_dir = Path("output/cross_evaluation_results")

    zero_score_evals = []
    stats_by_evaluator = defaultdict(int)
    stats_by_patient = defaultdict(int)

    # éåŽ†æ‰€æœ‰æ‚£è€…
    for patient_dir in sorted(base_dir.glob("æ‚£è€…*")):
        if not patient_dir.is_dir():
            continue

        patient = patient_dir.name
        eval_dir = patient_dir / "evaluations"

        if not eval_dir.exists():
            continue

        # éåŽ†æ‰€æœ‰è¯„ä¼°æ–‡ä»¶
        for eval_file in eval_dir.glob("*.json"):
            try:
                with open(eval_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)

                score = data.get('average_score', -1)

                # è¯†åˆ«é›¶åˆ†è¯„ä¼°
                if score == 0.0:
                    # è§£æžæ–‡ä»¶å: model_by_evaluator.json
                    filename = eval_file.stem
                    parts = filename.split('_by_')

                    if len(parts) == 2:
                        model = parts[0]
                        evaluator = parts[1]

                        zero_score_evals.append({
                            'patient': patient,
                            'model': model,
                            'evaluator': evaluator,
                            'file': str(eval_file)
                        })

                        stats_by_evaluator[evaluator] += 1
                        stats_by_patient[patient] += 1

            except Exception as e:
                print(f"Error reading {eval_file}: {e}")

    return zero_score_evals, stats_by_evaluator, stats_by_patient


def main():
    print("=" * 60)
    print("è¯†åˆ«é›¶åˆ†è¯„ä¼°")
    print("=" * 60)
    print()

    zero_evals, by_evaluator, by_patient = identify_zero_score_evaluations()

    print(f"ðŸ“Š æ€»è®¡: {len(zero_evals)} ä¸ªé›¶åˆ†è¯„ä¼°")
    print()

    # æŒ‰è¯„ä¼°è€…ç»Ÿè®¡
    print("æŒ‰è¯„ä¼°è€…åˆ†å¸ƒ:")
    for evaluator, count in sorted(by_evaluator.items(), key=lambda x: -x[1]):
        print(f"  {evaluator}: {count}")
    print()

    # æŒ‰æ‚£è€…ç»Ÿè®¡
    print("æŒ‰æ‚£è€…åˆ†å¸ƒ:")
    for patient, count in sorted(by_patient.items()):
        print(f"  {patient}: {count}")
    print()

    # ä¿å­˜é‡è¯•åˆ—è¡¨
    retry_list_file = "retry_zero_scores.json"
    with open(retry_list_file, 'w', encoding='utf-8') as f:
        json.dump(zero_evals, f, indent=2, ensure_ascii=False)

    print(f"âœ… é‡è¯•åˆ—è¡¨å·²ä¿å­˜åˆ°: {retry_list_file}")
    print()

    # æ˜¾ç¤ºå‰10ä¸ªç¤ºä¾‹
    print("å‰10ä¸ªç¤ºä¾‹:")
    for i, item in enumerate(zero_evals[:10], 1):
        print(f"  {i}. {item['patient']} - {item['model']} â†’ {item['evaluator']}")

    if len(zero_evals) > 10:
        print(f"  ... è¿˜æœ‰ {len(zero_evals) - 10} ä¸ª")

    return zero_evals


if __name__ == "__main__":
    main()
