#!/usr/bin/env python3
"""
äº¤å‰è¯„ä¼°ç³»ç»Ÿä¸»æ‰§è¡Œè„šæœ¬
è¿è¡Œæ¨¡å‹é—´çš„äº¤å‰è¯„ä¼°å¹¶ç”Ÿæˆè¯„åˆ†çŸ©é˜µ
"""

import argparse
import json
from pathlib import Path
from cross_evaluation.engine import CrossEvaluationEngine
from cross_evaluation.matrix import MatrixGenerator
from cross_evaluation.conversation_indexer import ConversationIndexer


def main():
    parser = argparse.ArgumentParser(description="è¿è¡Œæ¨¡å‹äº¤å‰è¯„ä¼°ç³»ç»Ÿ")

    parser.add_argument(
        "--patients",
        nargs="+",
        help="æŒ‡å®šè¦è¯„ä¼°çš„æ‚£è€…åˆ—è¡¨ï¼ˆå¦‚ï¼šæ‚£è€…1 æ‚£è€…2ï¼‰ï¼Œä¸æŒ‡å®šåˆ™è¯„ä¼°å…¨éƒ¨"
    )

    parser.add_argument(
        "--models",
        nargs="+",
        help="æŒ‡å®šè¦ä½¿ç”¨çš„æ¨¡å‹åˆ—è¡¨ï¼Œä¸æŒ‡å®šåˆ™ä½¿ç”¨å…¨éƒ¨æ¨¡å‹"
    )

    # æ³¨é‡Šï¼šä¸å†éœ€è¦conversationså‚æ•°ï¼Œå› ä¸ºç°åœ¨è¯„ä¼°çš„æ˜¯å®Œæ•´æŠ¥å‘Š
    # parser.add_argument(
    #     "--conversations",
    #     nargs="+",
    #     help="æŒ‡å®šè¦è¯„ä¼°çš„å¯¹è¯ç±»å‹IDåˆ—è¡¨ï¼ˆå¦‚ï¼š1 2 3ï¼‰ï¼Œä¸æŒ‡å®šåˆ™è¯„ä¼°å…¨éƒ¨"
    # )

    parser.add_argument(
        "--skip-evaluation",
        action="store_true",
        help="è·³è¿‡è¯„ä¼°ï¼Œä»…ç”ŸæˆçŸ©é˜µï¼ˆç”¨äºå·²æœ‰è¯„ä¼°ç»“æœçš„æƒ…å†µï¼‰"
    )

    parser.add_argument(
        "--output-dir",
        type=str,
        default="output/cross_evaluation_results",
        help="è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤: output/cross_evaluation_resultsï¼‰"
    )

    parser.add_argument(
        "--test-mode",
        action="store_true",
        help="æµ‹è¯•æ¨¡å¼ï¼šä»…è¯„ä¼°1ä¸ªæ‚£è€…çš„1ä¸ªå¯¹è¯"
    )

    args = parser.parse_args()

    print("\n" + "="*80)
    print("ğŸ¤– æ¨¡å‹äº¤å‰è¯„ä¼°ç³»ç»Ÿ")
    print("="*80 + "\n")

    # åŠ è½½å¯¹æ¯”æ•°æ®ä»¥è·å–å¯ç”¨çš„æ‚£è€…å’Œæ¨¡å‹
    comparison_data_path = Path("output/comparison_data.json")
    if not comparison_data_path.exists():
        print("âŒ é”™è¯¯: æœªæ‰¾åˆ° output/comparison_data.json")
        print("   è¯·å…ˆè¿è¡Œ prepare_comparison_data.py ç”Ÿæˆå¯¹æ¯”æ•°æ®")
        return

    with open(comparison_data_path, 'r', encoding='utf-8') as f:
        comparison_data = json.load(f)

    available_patients = comparison_data.get("patients", [])
    available_models = comparison_data.get("models", [])

    print("ğŸ“Š å¯ç”¨æ•°æ®:")
    print(f"   æ‚£è€…æ•°é‡: {len(available_patients)}")
    print(f"   æ¨¡å‹æ•°é‡: {len(available_models)}")
    print(f"   æ¨¡å‹åˆ—è¡¨: {', '.join(available_models)}\n")

    # ç¡®å®šè¯„ä¼°èŒƒå›´
    patients = args.patients or available_patients
    models = args.models or available_models

    # æµ‹è¯•æ¨¡å¼ï¼šåªè¯„ä¼°ç¬¬ä¸€ä¸ªæ‚£è€…
    if args.test_mode:
        print("âš ï¸  æµ‹è¯•æ¨¡å¼ï¼šä»…è¯„ä¼°ç¬¬ä¸€ä¸ªæ‚£è€…çš„å®Œæ•´æŠ¥å‘Š\n")
        patients = patients[:1]

    print("ğŸ¯ è¯„ä¼°èŒƒå›´:")
    print(f"   æ‚£è€…: {', '.join(patients)}")
    print(f"   æ¨¡å‹: {', '.join(models)}")
    print(f"   è¯„ä¼°ç±»å‹: å®Œæ•´æŠ¥å‘Šï¼ˆæ‰€æœ‰å¯¹è¯åˆå¹¶ï¼‰")
    print()

    # æ­¥éª¤1: è¿è¡Œäº¤å‰è¯„ä¼°ï¼ˆå®Œæ•´æŠ¥å‘Šï¼‰
    if not args.skip_evaluation:
        print("="*80)
        print("æ­¥éª¤ 1/3: è¿è¡Œå®Œæ•´æŠ¥å‘Šäº¤å‰è¯„ä¼°")
        print("="*80 + "\n")

        engine = CrossEvaluationEngine(
            output_base_dir=args.output_dir,
            markdown_reports_dir="output/markdown"
        )

        results = engine.run_report_cross_evaluation(
            æ‚£è€…åˆ—è¡¨=patients,
            æ¨¡å‹åˆ—è¡¨=models
        )

        print("\nâœ… äº¤å‰è¯„ä¼°å®Œæˆ")
        print(f"   æˆåŠŸ: {results['statistics']['successful']}")
        print(f"   å¤±è´¥: {results['statistics']['failed']}")
    else:
        print("â­ï¸  è·³è¿‡è¯„ä¼°æ­¥éª¤ï¼ˆä½¿ç”¨å·²æœ‰ç»“æœï¼‰\n")

    # æ­¥éª¤2: ç”Ÿæˆè¯„åˆ†çŸ©é˜µ
    print("\n" + "="*80)
    print("æ­¥éª¤ 2/3: ç”Ÿæˆè¯„åˆ†çŸ©é˜µ")
    print("="*80 + "\n")

    matrix_gen = MatrixGenerator(evaluation_base_dir=args.output_dir)
    all_matrices = matrix_gen.generate_all_matrices(patients, models)

    print(f"\nâœ… å·²ç”Ÿæˆ {sum(len(p) for p in all_matrices.values())} ä¸ªçŸ©é˜µ")

    # æ­¥éª¤3: ç”Ÿæˆæ±‡æ€»ç»Ÿè®¡
    print("\n" + "="*80)
    print("æ­¥éª¤ 3/3: ç”Ÿæˆæ±‡æ€»ç»Ÿè®¡")
    print("="*80 + "\n")

    summary = matrix_gen.generate_summary_statistics(all_matrices)

    # æ˜¾ç¤ºæ€»ä½“æ’å
    print("\nğŸ† æ¨¡å‹æ€»ä½“æ’å:")
    print("-" * 60)
    for rank_data in summary["overall_rankings"][:10]:  # æ˜¾ç¤ºå‰10å
        rank = rank_data["rank"]
        model = rank_data["model"]
        mean = rank_data["mean"]
        std = rank_data["std"]
        count = rank_data["count"]

        print(f"  {rank:2d}. {model:30s} - å¹³å‡åˆ†: {mean:.2f} (Â±{std:.2f}, n={count})")

    print("\n" + "="*80)
    print("âœ¨ å®Œæ•´æŠ¥å‘Šäº¤å‰è¯„ä¼°ç³»ç»Ÿè¿è¡Œå®Œæˆï¼")
    print("="*80)
    print(f"\nğŸ“ ç»“æœä¿å­˜ä½ç½®: {args.output_dir}/")
    print("\nğŸ“Š æŸ¥çœ‹ç»“æœ:")
    print("   - åŸå§‹è¯„ä¼°å“åº”: {output_dir}/{æ‚£è€…}/raw/")
    print("   - è§£æåè¯„ä¼°: {output_dir}/{æ‚£è€…}/evaluations/")
    print("   - è¯„åˆ†çŸ©é˜µ: {output_dir}/{æ‚£è€…}/matrix.json")
    print("   - æ±‡æ€»ç»Ÿè®¡: {output_dir}/summary/statistics.json")
    print("\nğŸ’¡ ä¸‹ä¸€æ­¥:")
    print("   - è¿è¡Œ python parse_raw_responses.py è§£æåŸå§‹å“åº”")
    print("   - æ‰“å¼€ cross_evaluation_viewer.html å¯è§†åŒ–æŸ¥çœ‹ç»“æœ")
    print("   - ä½¿ç”¨ --test-mode å¿«é€Ÿæµ‹è¯•ç³»ç»Ÿ")
    print("="*80 + "\n")


if __name__ == "__main__":
    main()
