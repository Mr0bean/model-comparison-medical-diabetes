#!/usr/bin/env python3
"""
ç”Ÿæˆå‰ç«¯æ‰€éœ€çš„æ•°æ®æ–‡ä»¶
ä»äº¤å‰è¯„æµ‹ç»“æœå’ŒåŸå§‹æŠ¥å‘Šç”Ÿæˆå‰ç«¯JSONæ–‡ä»¶
"""
import json
from pathlib import Path
from collections import defaultdict
from datetime import datetime
import statistics

# é…ç½®
RESULTS_DIR = Path("output/cross_evaluation_results")
RAW_DIR = Path("output/raw")
OUTPUT_DIR = Path("output")
CONFIG_FILE = Path("config/cross_evaluation_config.json")

def load_config():
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    with open(CONFIG_FILE, 'r', encoding='utf-8') as f:
        return json.load(f)

def load_raw_reports():
    """åŠ è½½æ‰€æœ‰åŸå§‹æŠ¥å‘Š"""
    reports = {}
    for report_file in RAW_DIR.glob("*.json"):
        # æ–‡ä»¶åæ ¼å¼: {æ¨¡å‹}-{æ‚£è€…}.json
        parts = report_file.stem.split('-')
        if len(parts) < 2:
            continue

        patient = parts[-1]  # æœ€åä¸€éƒ¨åˆ†æ˜¯æ‚£è€…
        model = '-'.join(parts[:-1])  # å‰é¢çš„éƒ¨åˆ†æ˜¯æ¨¡å‹å

        try:
            with open(report_file, 'r', encoding='utf-8') as f:
                data = json.load(f)

            # æå–å…³é”®ä¿¡æ¯
            reports[f"{model}_{patient}"] = {
                "model": model,
                "patient": patient,
                "conversations": data.get("conversations", {}),
                "result": data.get("result", ""),
                "people": data.get("people", {})
            }
        except Exception as e:
            print(f"âš ï¸  åŠ è½½æŠ¥å‘Šå¤±è´¥ {report_file}: {e}")

    return reports

def load_evaluation_results():
    """åŠ è½½æ‰€æœ‰è¯„æµ‹ç»“æœ"""
    evaluations = []

    # éå†æ‰€æœ‰æ‚£è€…ç›®å½•
    for patient_dir in sorted(RESULTS_DIR.glob("æ‚£è€…*")):
        patient = patient_dir.name

        # æŸ¥æ‰¾æ‰€æœ‰èšåˆæ–‡ä»¶
        for agg_file in patient_dir.glob("*_aggregated.json"):
            try:
                with open(agg_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)

                evaluations.append({
                    "evaluated_model": data.get("evaluated_model"),
                    "evaluator_model": data.get("evaluator_model"),
                    "patient": patient,
                    "total_score": data.get("total_score", 0),
                    "max_total_score": data.get("max_total_score", 100),
                    "dimensions": data.get("dimensions", {}),
                    "critical_feedbacks": data.get("critical_feedbacks", []),
                    "timestamp": data.get("timestamp")
                })
            except Exception as e:
                print(f"âš ï¸  åŠ è½½è¯„æµ‹å¤±è´¥ {agg_file}: {e}")

    return evaluations

def generate_cross_evaluation_matrix(evaluations, models, patients):
    """ç”Ÿæˆäº¤å‰è¯„æµ‹çŸ©é˜µæ•°æ®"""
    print("\nğŸ“Š ç”Ÿæˆäº¤å‰è¯„æµ‹çŸ©é˜µ...")

    # å…¨å±€çŸ©é˜µ
    global_matrix = defaultdict(lambda: defaultdict(lambda: {
        "scores": [],
        "count": 0
    }))

    # æŒ‰æ‚£è€…åˆ†ç»„çš„çŸ©é˜µ
    patient_matrices = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: {
        "scores": [],
        "count": 0
    })))

    # èšåˆæ•°æ®
    for eval_data in evaluations:
        evaluated = eval_data["evaluated_model"]
        evaluator = eval_data["evaluator_model"]
        patient = eval_data["patient"]
        score = eval_data["total_score"]

        # å…¨å±€çŸ©é˜µ
        global_matrix[evaluator][evaluated]["scores"].append(score)
        global_matrix[evaluator][evaluated]["count"] += 1

        # æ‚£è€…çŸ©é˜µ
        patient_matrices[patient][evaluator][evaluated]["scores"].append(score)
        patient_matrices[patient][evaluator][evaluated]["count"] += 1

    # è®¡ç®—ç»Ÿè®¡å€¼
    def compute_stats(data_dict):
        result = {}
        for evaluator in data_dict:
            result[evaluator] = {}
            for evaluated in data_dict[evaluator]:
                scores = data_dict[evaluator][evaluated]["scores"]
                if scores:
                    result[evaluator][evaluated] = {
                        "score": round(statistics.mean(scores), 2),
                        "count": len(scores),
                        "stddev": round(statistics.stdev(scores), 2) if len(scores) > 1 else 0,
                        "min": min(scores),
                        "max": max(scores),
                        "details": scores
                    }
        return result

    global_stats = compute_stats(global_matrix)
    patient_stats = {p: compute_stats(patient_matrices[p]) for p in patient_matrices}

    # ç”Ÿæˆå®Œæ•´çŸ©é˜µæ•°æ®
    matrix_data = {
        "metadata": {
            "generated_at": datetime.now().isoformat(),
            "total_evaluations": len(evaluations),
            "evaluation_type": "100-point",
            "dimensions": [
                {"name": "å‡†ç¡®æ€§", "max_points": 40, "weight": 0.4},
                {"name": "é€»è¾‘æ€§", "max_points": 25, "weight": 0.25},
                {"name": "å®Œæ•´æ€§", "max_points": 15, "weight": 0.15},
                {"name": "æ ¼å¼è§„èŒƒæ€§", "max_points": 15, "weight": 0.15},
                {"name": "è¯­è¨€è¡¨è¾¾", "max_points": 5, "weight": 0.05}
            ]
        },
        "models": models,
        "patients": patients,
        "global_matrix": global_stats,
        "matrices": patient_stats
    }

    # ä¿å­˜æ–‡ä»¶
    output_file = OUTPUT_DIR / "cross_evaluation_matrix.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(matrix_data, f, ensure_ascii=False, indent=2)

    print(f"âœ… äº¤å‰è¯„æµ‹çŸ©é˜µå·²ä¿å­˜: {output_file}")
    print(f"   - æ€»è¯„æµ‹æ•°: {len(evaluations)}")
    print(f"   - æ¨¡å‹æ•°: {len(models)}")
    print(f"   - æ‚£è€…æ•°: {len(patients)}")

    return matrix_data

def generate_comparison_data(reports, models, patients):
    """ç”Ÿæˆæ¨¡å‹å¯¹æ¯”æ•°æ®"""
    print("\nğŸ“Š ç”Ÿæˆæ¨¡å‹å¯¹æ¯”æ•°æ®...")

    # å‚è€ƒæ¨¡å‹ï¼ˆç”¨äºæå–æ ‡é¢˜ï¼‰
    REFERENCE_MODEL = "gemini-3-pro-preview"

    # é‡ç»„æ•°æ®ç»“æ„
    comparison_data = {
        "metadata": {
            "generated_at": datetime.now().isoformat(),
            "total_reports": len(reports),
            "models_count": len(models),
            "patients_count": len(patients),
            "reference_model": REFERENCE_MODEL
        },
        "models": models,
        "patients": patients,
        "data": {}
    }

    # ç¬¬ä¸€æ­¥ï¼šæ”¶é›†å‚è€ƒæ¨¡å‹çš„æ ‡é¢˜
    reference_titles = {}  # {patient: {conv_id: title}}

    for key, report in reports.items():
        patient = report["patient"]
        model = report["model"]

        # ä»å‚è€ƒæ¨¡å‹æå–æ ‡é¢˜
        if model == REFERENCE_MODEL:
            conversations = report["conversations"]
            if patient not in reference_titles:
                reference_titles[patient] = {}

            for conv_id, conv_data in conversations.items():
                # ä»Outputå­—æ®µæå–æ ‡é¢˜ï¼ˆOutputçš„ç¬¬ä¸€è¡Œé€šå¸¸æ˜¯æ ‡é¢˜ï¼‰
                output = conv_data.get("Output", "")
                # æ¸…ç†æ‰€æœ‰æ¢è¡Œç¬¦ï¼Œç„¶åè·å–ç¬¬ä¸€è¡Œ
                cleaned_output = output.replace('\\n', '').replace('\n', ' ').strip() if output else ""

                # å¦‚æœæ¸…ç†åçš„æ–‡æœ¬å¤ªé•¿ï¼ˆè¶…è¿‡30å­—ï¼‰ï¼Œæˆªå–å‰30å­—å¹¶æ·»åŠ çœç•¥å·
                if cleaned_output and len(cleaned_output) <= 30:
                    title = cleaned_output
                elif cleaned_output and len(cleaned_output) > 30:
                    title = cleaned_output[:30] + "..."
                else:
                    title = f"å¯¹è¯{conv_id}"

                reference_titles[patient][conv_id] = title

    print(f"âœ… ä»å‚è€ƒæ¨¡å‹ {REFERENCE_MODEL} æå–äº†æ ‡é¢˜")

    # ç¬¬äºŒæ­¥ï¼šæŒ‰æ‚£è€…åˆ†ç»„ï¼Œä½¿ç”¨å‚è€ƒæ ‡é¢˜
    for key, report in reports.items():
        patient = report["patient"]
        model = report["model"]
        conversations = report["conversations"]

        if patient not in comparison_data["data"]:
            comparison_data["data"][patient] = {}

        # æŒ‰å¯¹è¯è½®æ¬¡ç»„ç»‡
        for conv_id, conv_data in conversations.items():
            if conv_id not in comparison_data["data"][patient]:
                comparison_data["data"][patient][conv_id] = {}

            # ä½¿ç”¨å‚è€ƒæ¨¡å‹çš„æ ‡é¢˜ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨é»˜è®¤æ ‡é¢˜
            title = reference_titles.get(patient, {}).get(conv_id, f"å¯¹è¯{conv_id}")

            comparison_data["data"][patient][conv_id][model] = {
                "title": title,
                "output": conv_data.get("Output", ""),
                "chat": conv_data.get("chat", "")
            }

    # ä¿å­˜æ–‡ä»¶
    output_file = OUTPUT_DIR / "comparison_data.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(comparison_data, f, ensure_ascii=False, indent=2)

    print(f"âœ… æ¨¡å‹å¯¹æ¯”æ•°æ®å·²ä¿å­˜: {output_file}")
    print(f"   - æŠ¥å‘Šæ•°: {len(reports)}")

    return comparison_data

def generate_evaluation_details(evaluations, reports):
    """ç”Ÿæˆè¯¦ç»†è¯„æµ‹æ•°æ®ï¼ˆç”¨äºè¯„æµ‹é¡µé¢ï¼‰"""
    print("\nğŸ“Š ç”Ÿæˆè¯¦ç»†è¯„æµ‹æ•°æ®...")

    details_data = {
        "metadata": {
            "generated_at": datetime.now().isoformat(),
            "total_evaluations": len(evaluations)
        },
        "evaluations": []
    }

    for eval_data in evaluations:
        model = eval_data["evaluated_model"]
        patient = eval_data["patient"]

        # æŸ¥æ‰¾å¯¹åº”çš„æŠ¥å‘Š
        report_key = f"{model}_{patient}"
        report = reports.get(report_key, {})

        details_data["evaluations"].append({
            "model": model,
            "patient": patient,
            "evaluator": eval_data["evaluator_model"],
            "scores": {
                "dimensions": [
                    {
                        "name": dim_name,
                        "score": dim_data.get("score", 0),
                        "max_score": dim_data.get("max_score", 0),
                        "issues": dim_data.get("issues", "")
                    }
                    for dim_name, dim_data in eval_data["dimensions"].items()
                ],
                "total_score": eval_data["total_score"],
                "max_total_score": eval_data["max_total_score"]
            },
            "feedbacks": eval_data.get("critical_feedbacks", []),
            "report": report.get("result", ""),
            "conversations": report.get("conversations", {}),
            "timestamp": eval_data.get("timestamp")
        })

    # ä¿å­˜æ–‡ä»¶
    output_file = OUTPUT_DIR / "evaluation_details.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(details_data, f, ensure_ascii=False, indent=2)

    print(f"âœ… è¯¦ç»†è¯„æµ‹æ•°æ®å·²ä¿å­˜: {output_file}")

    return details_data

def generate_statistics(evaluations, models, patients):
    """ç”Ÿæˆç»Ÿè®¡æ•°æ®ï¼ˆç”¨äºé¦–é¡µï¼‰"""
    print("\nğŸ“Š ç”Ÿæˆç»Ÿè®¡æ•°æ®...")

    # è®¡ç®—å„ç§ç»Ÿè®¡æŒ‡æ ‡
    all_scores = [e["total_score"] for e in evaluations]

    # æ¨¡å‹æ’å
    model_scores = defaultdict(list)
    for eval_data in evaluations:
        model_scores[eval_data["evaluated_model"]].append(eval_data["total_score"])

    model_rankings = []
    for model, scores in model_scores.items():
        if scores:
            model_rankings.append({
                "model": model,
                "avg_score": round(statistics.mean(scores), 2),
                "min_score": min(scores),
                "max_score": max(scores),
                "stddev": round(statistics.stdev(scores), 2) if len(scores) > 1 else 0,
                "count": len(scores)
            })

    model_rankings.sort(key=lambda x: x["avg_score"], reverse=True)

    # è¯„æµ‹è€…ç‰¹å¾
    evaluator_stats = defaultdict(list)
    for eval_data in evaluations:
        evaluator_stats[eval_data["evaluator_model"]].append(eval_data["total_score"])

    evaluator_features = []
    for evaluator, scores in evaluator_stats.items():
        if scores:
            evaluator_features.append({
                "evaluator": evaluator,
                "avg_given_score": round(statistics.mean(scores), 2),
                "count": len(scores),
                "min_score": min(scores),
                "max_score": max(scores),
                "stddev": round(statistics.stdev(scores), 2) if len(scores) > 1 else 0
            })

    evaluator_features.sort(key=lambda x: x["avg_given_score"], reverse=True)

    # è¯„åˆ†åˆ†å¸ƒ
    score_distribution = {
        "0-20": 0,
        "21-40": 0,
        "41-60": 0,
        "61-80": 0,
        "81-100": 0
    }

    for score in all_scores:
        if score <= 20:
            score_distribution["0-20"] += 1
        elif score <= 40:
            score_distribution["21-40"] += 1
        elif score <= 60:
            score_distribution["41-60"] += 1
        elif score <= 80:
            score_distribution["61-80"] += 1
        else:
            score_distribution["81-100"] += 1

    statistics_data = {
        "metadata": {
            "generated_at": datetime.now().isoformat()
        },
        "overview": {
            "total_models": len(models),
            "total_patients": len(patients),
            "total_evaluations": len(evaluations),
            "total_files": len(evaluations) * 6,  # æ¯ä¸ªè¯„æµ‹6ä¸ªæ–‡ä»¶
            "completion_rate": 100.0
        },
        "scores": {
            "average": round(statistics.mean(all_scores), 2) if all_scores else 0,
            "median": round(statistics.median(all_scores), 2) if all_scores else 0,
            "min": min(all_scores) if all_scores else 0,
            "max": max(all_scores) if all_scores else 0,
            "stddev": round(statistics.stdev(all_scores), 2) if len(all_scores) > 1 else 0
        },
        "distribution": score_distribution,
        "model_rankings": model_rankings,
        "evaluator_features": evaluator_features
    }

    # ä¿å­˜æ–‡ä»¶
    output_file = OUTPUT_DIR / "statistics.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(statistics_data, f, ensure_ascii=False, indent=2)

    print(f"âœ… ç»Ÿè®¡æ•°æ®å·²ä¿å­˜: {output_file}")
    print(f"   - å¹³å‡åˆ†: {statistics_data['scores']['average']}")
    print(f"   - æœ€é«˜åˆ†: {statistics_data['scores']['max']}")
    print(f"   - æœ€ä½åˆ†: {statistics_data['scores']['min']}")

    return statistics_data

def main():
    print("=" * 80)
    print("ğŸ”„ å¼€å§‹ç”Ÿæˆå‰ç«¯æ•°æ®æ–‡ä»¶")
    print("=" * 80)

    # åŠ è½½é…ç½®
    config = load_config()
    models = config["models"]
    patients = config["patients"]

    print(f"\nğŸ“‹ é…ç½®ä¿¡æ¯:")
    print(f"   - æ¨¡å‹æ•°: {len(models)}")
    print(f"   - æ‚£è€…æ•°: {len(patients)}")

    # åŠ è½½åŸå§‹æ•°æ®
    print("\nğŸ“‚ åŠ è½½åŸå§‹æ•°æ®...")
    reports = load_raw_reports()
    evaluations = load_evaluation_results()

    print(f"   - åŸå§‹æŠ¥å‘Š: {len(reports)}")
    print(f"   - è¯„æµ‹ç»“æœ: {len(evaluations)}")

    # ç”Ÿæˆå„ç§æ•°æ®æ–‡ä»¶
    generate_cross_evaluation_matrix(evaluations, models, patients)
    generate_comparison_data(reports, models, patients)
    generate_evaluation_details(evaluations, reports)
    generate_statistics(evaluations, models, patients)

    print("\n" + "=" * 80)
    print("âœ… æ‰€æœ‰å‰ç«¯æ•°æ®æ–‡ä»¶ç”Ÿæˆå®Œæˆï¼")
    print("=" * 80)
    print("\nç”Ÿæˆçš„æ–‡ä»¶:")
    print("  ğŸ“„ output/cross_evaluation_matrix.json - äº¤å‰è¯„æµ‹çŸ©é˜µ")
    print("  ğŸ“„ output/comparison_data.json - æ¨¡å‹å¯¹æ¯”æ•°æ®")
    print("  ğŸ“„ output/evaluation_details.json - è¯¦ç»†è¯„æµ‹æ•°æ®")
    print("  ğŸ“„ output/statistics.json - ç»Ÿè®¡æ•°æ®")
    print()

if __name__ == "__main__":
    main()
