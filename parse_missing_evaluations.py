#!/usr/bin/env python3
"""è§£æå·²æœ‰rawæ–‡ä»¶ä½†æœªè§£æçš„è¯„ä¼°ç»“æœ"""

import json
from pathlib import Path
from cross_evaluation.engine import CrossEvaluationEngine

def find_missing_parsed_files():
    """æ‰¾åˆ°æ‰€æœ‰éœ€è¦è§£æçš„rawæ–‡ä»¶"""
    base_dir = Path("output/cross_evaluation_results")
    missing = []

    for patient_dir in sorted(base_dir.glob("æ‚£è€…*")):
        patient = patient_dir.name
        raw_dir = patient_dir / "raw"
        eval_dir = patient_dir / "evaluations"

        if not raw_dir.exists():
            continue

        # æ£€æŸ¥æ¯ä¸ªrawæ–‡ä»¶
        for raw_file in raw_dir.glob("*.json"):
            eval_file = eval_dir / raw_file.name

            if not eval_file.exists():
                missing.append({
                    "patient": patient,
                    "raw_file": raw_file,
                    "eval_file": eval_file
                })

    return missing

def parse_raw_file(raw_file_path, eval_file_path, engine):
    """è§£æå•ä¸ªrawæ–‡ä»¶"""
    try:
        # è¯»å–rawæ–‡ä»¶
        with open(raw_file_path, 'r', encoding='utf-8') as f:
            raw_data = json.load(f)

        patient = raw_data.get("patient")
        generated_by = raw_data.get("generated_by")
        evaluated_by = raw_data.get("evaluated_by")

        # è°ƒç”¨è§£æå‡½æ•°
        result = engine._parse_and_save_report_evaluation(
            patient=patient,
            generated_by=generated_by,
            evaluated_by=evaluated_by,
            raw_response=raw_data
        )

        return True, result.get("average_score", 0)

    except Exception as e:
        return False, str(e)

def main():
    print("=" * 80)
    print("ğŸ”§ è§£ææœªå¤„ç†çš„Rawæ–‡ä»¶")
    print("=" * 80)
    print()

    # æŸ¥æ‰¾ç¼ºå¤±çš„æ–‡ä»¶
    missing = find_missing_parsed_files()

    print(f"ğŸ“Š æ‰¾åˆ° {len(missing)} ä¸ªéœ€è¦è§£æçš„rawæ–‡ä»¶\n")

    if not missing:
        print("âœ… æ‰€æœ‰rawæ–‡ä»¶éƒ½å·²è§£æå®Œæˆï¼")
        return

    # åˆå§‹åŒ–å¼•æ“
    engine = CrossEvaluationEngine(
        output_base_dir="output/cross_evaluation_results",
        markdown_reports_dir="output/markdown"
    )

    # è§£ææ¯ä¸ªæ–‡ä»¶
    success_count = 0
    failed_count = 0

    for i, item in enumerate(missing, 1):
        patient = item["patient"]
        raw_file = item["raw_file"]
        eval_file = item["eval_file"]

        print(f"[{i}/{len(missing)}] {patient} - {raw_file.name}")

        success, result = parse_raw_file(raw_file, eval_file, engine)

        if success:
            print(f"    âœ“ æˆåŠŸè§£æï¼Œå¹³å‡åˆ†: {result:.2f}")
            success_count += 1
        else:
            print(f"    âœ— è§£æå¤±è´¥: {result}")
            failed_count += 1

        print()

    print("=" * 80)
    print(f"âœ¨ è§£æå®Œæˆï¼")
    print(f"   æˆåŠŸ: {success_count}")
    print(f"   å¤±è´¥: {failed_count}")
    print("=" * 80)

if __name__ == "__main__":
    main()
